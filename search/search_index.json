{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenShift on IBM Power Extras! Utility Scripts ocp-upgrade-paths.sh - lists available OCP upgrade paths setup_nfs_provisioner.sh - sets up a NFS share on the system, exports it, installs NFS provisioner pod to support dynamically provision PVs on NFS KnowledgeBase How to troubleshoot... Pending Pods Resource Exhaustion situation Security Context (SCC) issues How to manage Node health Prometheus Queries for monitoring of oc cluster External pages that helped resolve issues that we actually faced FAQ on PowerVS deployments Deploying kruize on Openshift on Power and more to come!","title":"Home(grown)"},{"location":"#welcome-to-openshift-on-ibm-power-extras","text":"","title":"Welcome to OpenShift on IBM Power Extras!"},{"location":"#utility-scripts","text":"ocp-upgrade-paths.sh - lists available OCP upgrade paths setup_nfs_provisioner.sh - sets up a NFS share on the system, exports it, installs NFS provisioner pod to support dynamically provision PVs on NFS","title":"Utility Scripts"},{"location":"#knowledgebase","text":"How to troubleshoot... Pending Pods Resource Exhaustion situation Security Context (SCC) issues How to manage Node health Prometheus Queries for monitoring of oc cluster External pages that helped resolve issues that we actually faced FAQ on PowerVS deployments Deploying kruize on Openshift on Power and more to come!","title":"KnowledgeBase"},{"location":"Node_Health_Management/","text":"Managing Node Health A node provides the runtime environments for containers. Each node in a Kubernetes cluster has the required services to be managed by the master. Nodes also have the required services to run pods, including the container runtime, a kubelet, and a service proxy. OpenShift Container Platform creates nodes from a cloud provider, physical systems, or virtual systems. Kubernetes interacts with node objects that are a representation of those nodes. The master uses the information from node objects to validate nodes with health checks. A node is ignored until it passes the health checks, and the master continues checking nodes until they are valid. We can manage nodes in our instance using the CLI. When we perform node management operations, the CLI interacts with node objects that are representations of actual node hosts. The master uses the information from node objects to validate nodes with health checks. You can review cluster node health status, resource consumption statistics, and node logs using the below commands. Additionally, you can query kubelet status on individual nodes. 1. Node reporting NotReady The mentioned problem was seen on 4.5.4 running on KVM/libvirt, but this could happen and be applied on other platforms. Some nodes in this 3-master/5-worker cluster start showing \"NotReady\" state with \"Kubelet stopped posting node status\" in oc describe node output. SSH'ing into those \"NotReady\" nodes, it could be seen that Current certificate is expired in journalctl -u kubelet , and that is precisely the issue preventing communication with API server. You might also see: Failed Units: 1 NetworkManager-wait-online.service [core@worker-0 ~]$ upon SSH login, but that is a red herring. In order to resolve this issue run the following commands on \"NotReady\" node(s): $ sudo systemctl restart kubelet followed by, anywhere you can run oc commands against the cluster; $ oc get csr | grep Pending | awk '{print $1}' | xargs oc adm certificate approve Note - Make sure you have no more \"Pending\" CSRs after a minute or two as well. 2. Querying the kubelet\u2019s status on a node The kubelet is managed using a systemd service on each node. Review the kubelet\u2019s status by querying the kubelet systemd service within a debug Pod. a. Start a debug Pod for a node: $ oc debug node/node_name b. Set /host as the root directory within the debug shell. The debug Pod mounts the host\u2019s root file system in /host within the Pod. By changing the root directory to /host, you can run binaries contained in the host\u2019s executable paths: # chroot /host c. Check whether the kubelet systemd service is active on the node: # systemctl is-active kubelet d. Output a more detailed kubelet.service status summary: # systemctl status kubelet Status on CLI oc debug node/master0.ocp-support-suad.cp.fyre.ibm.com Starting pod/master0ocp-support-suadcpfyreibmcom-debug ... To use host binaries, run `chroot /host` Pod IP: 10.17.73.96 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# systemctl is-active kubelet active sh-4.4# systemctl status kubelet \u25cf kubelet.service - MCO environment configuration Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d \u2514\u250010-mco-default-env.conf Active: active (running) since Thu 2020-08-27 07:47:12 UTC; 1 day 3h ago Process: 1347 ExecStartPre=/bin/rm -f /var/lib/kubelet/cpu_manager_state (code=exited, status=0/SUCCESS) Process: 1344 ExecStartPre=/bin/mkdir --parents /etc/kubernetes/manifests (code=exited, status=0/SUCCESS) Main PID: 1349 (kubelet) Tasks: 38 (limit: 51304) Memory: 264.3M CPU: 7h 15min 31.395s CGroup: /system.slice/kubelet.service \u2514\u25001349 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-en> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827568 1349 atomic_writer.go:157] pod openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-769d7b> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827609 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"config\" (UniqueName: \"kubernetes> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827720 1349 secret.go:183] Setting up volume node-ca-token-r9zm6 for pod 42e3fbda-1361-4c75-9f72-7938a75a6073 at> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827792 1349 secret.go:207] Received secret openshift-image-registry/node-ca-token-r9zm6 containing (4) pieces of> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828220 1349 atomic_writer.go:157] pod openshift-image-registry/node-ca-vhm68 volume node-ca-token-r9zm6: no upda> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828265 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"node-ca-token-r9zm6\" (UniqueName> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828357 1349 secret.go:183] Setting up volume serving-cert for pod a3679c68-645c-4b58-9324-b03a923624f0 at /var/l> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828419 1349 secret.go:207] Received secret openshift-kube-scheduler-operator/kube-scheduler-operator-serving-cer> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828681 1349 atomic_writer.go:157] pod openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-769d7b> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828717 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"serving-cert\" (UniqueName: \"kube> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.837829 1349 atomic_writer.go:157] pod openshift-image-registry/node-ca-vhm68 volume serviceca: no update require> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.837926 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"serviceca\" (UniqueName: \"kuberne> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.970203 1349 exec.go:60] Exec probe response: \"\" Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.970272 1349 prober.go:133] Liveness probe for \"ovs-p2qkz_openshift-sdn(6d62e5db-053b-4573-95ce-d7d478f578d9):ope> 3. Node-logs: Get logs for NetworkManager The below command will give the logs of the network manager of a particular node of a cluster. $ oc adm node-logs --role master -u NetworkManager.service Sep 06 04:01:37.044137 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0439] dhcp4 (enp0s3): option dhcp_lease_time => '600' Sep 06 04:01:37.044177 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0441] dhcp4 (enp0s3): option domain_name => 'cp.fyre.ibm.com' Sep 06 04:01:37.044199 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0441] dhcp4 (enp0s3): option domain_name_servers => '10.17.64.21 10.17.64.22' Sep 06 04:01:37.044219 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option domain_search => 'cp.fyre.ibm.com' Sep 06 04:01:37.044239 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option expiry => '1599365497' Sep 06 04:01:37.044259 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option ip_address => '10.17.73.158' Sep 06 04:01:37.044280 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option next_server => '10.17.64.9' Sep 06 04:01:37.044301 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option requested_broadcast_address => '1' Sep 06 04:01:37.044322 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_domain_name => '1' Sep 06 04:01:37.044342 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_domain_name_servers => '1' Sep 06 04:01:37.044363 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_domain_search => '1' Sep 06 04:01:37.044383 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_host_name => '1' Sep 06 04:01:37.044403 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_interface_mtu => '1' Sep 06 04:01:37.044424 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_ms_classless_static_routes => '1' Sep 06 04:01:37.044471 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_nis_domain => '1' Sep 06 04:01:37.044491 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_nis_servers => '1' Sep 06 04:01:37.044511 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_ntp_servers => '1' Sep 06 04:01:37.044532 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_rfc3442_classless_static_routes => '1' Sep 06 04:01:37.044552 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_root_path => '1' Sep 06 04:01:37.044572 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_routers => '1' Sep 06 04:01:37.044593 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_static_routes => '1' Sep 06 04:01:37.044613 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_subnet_mask => '1' Sep 06 04:01:37.044637 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0446] dhcp4 (enp0s3): option requested_time_offset => '1' Sep 06 04:01:37.044658 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0446] dhcp4 (enp0s3): option requested_wpad => '1' Sep 06 04:01:37.044734 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0446] dhcp4 (enp0s3): option routers => '10.17.64.1' Sep 06 04:01:37.044759 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0447] dhcp4 (enp0s3): option subnet_mask => '255.255.224.0' Sep 06 04:01:37.044779 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0447] dhcp4 (enp0s3): state changed extended -> extended 4. Nodes report ready but ETCD health check fails [root@master02 ~]# etcdctl -C https://master00.ose.example.com:2379,https://master01.ose.example.com:2379,https://master01.ose.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health member e0e2c123213680f is healthy: got healthy result from https://192.168.200.50:2379 member 64f1077d838e039c is healthy: got healthy result from https://192.168.200.51:2379 member a9e031ea9ce2a521 is unhealthy: got unhealthy result from https://192.168.200.52:2379 In the event that the health check fails check the status of etcd you could see one or a combination of the following: [root@master02 ~]# etcdctl -C https://master00.ose.example.com:2379,https://master01.ose.example.com:2379,https://master01.ose.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health member e0e2c123213680f is healthy: got healthy result from https://192.168.200.50:2379 member 64f1077d838e039c is healthy: got healthy result from https://192.168.200.51:2379 member a9e031ea9ce2a521 is unhealthy: got unhealthy result from https://192.168.200.52:2379 [root@master01 ~]# systemctl status etcd \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2016-02-25 08:43:37 CST; 4h 32min ago Main PID: 1103 (etcd) CGroup: /system.slice/etcd.service \u2514\u25001103 /usr/bin/etcd --name=master01.ose.example.com --data-dir=/var/lib/etcd/ --lis... Feb 25 11:32:52 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:32:52 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:02 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:02 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:12 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:12 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) [root@master00 ~]# systemctl status etcd \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2016-02-25 08:43:55 CST; 4h 32min ago Main PID: 1097 (etcd) CGroup: /system.slice/etcd.service \u2514\u25001097 /usr/bin/etcd --name=master00.ose.example.com --data-dir=/var/lib/etcd/ --lis... Feb 25 11:40:25 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:40:55 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:41:25 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:41:55 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:42:25 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Solution In most cases restarting etcd one at a time on each etcd host resolves the issue $ sudo systemctl restart etcd Reference Link - http://v1.uncontained.io/playbooks/troubleshooting/troubleshooting_guide.html 5. File System tracking(a complete monitoring for disk usage) To monitor filesystems and more precisely the overall space remaining on various filesystems , the node exporter exports two metrics to retrieve such statistics : a. node_filesystem_avail_bytes b. node_filesystem_size_bytes Use below query in Prometheus for the overall filesystem usage by device or by mountpoint : ( 1 - node_filesystem_avail_bytes /node_filesystem_size_bytes )*100 6. Read & Write Latencies Another great metric for monitoring is the read and write latencies on our disks. The node exporter exports multiple metrics to compute it. On the read side, we have: a. node_disk_read_time_seconds_total b. node_disk_reads_completed_total On the write side, we have: a. node_disk_write_time_seconds_total b. node_disk_writes_completed_total If we compute rates for the two metrics, and divide one by the other, we are able to compute the latency or the time that your disk takes in order to complete such operations. Use below query in Prometheus for the disk latency : rate(node_disk_write_time_seconds_total[5s])/rate(node_disk_writes_completed_total[5s])*100","title":"Managing Node Health"},{"location":"Node_Health_Management/#managing-node-health","text":"A node provides the runtime environments for containers. Each node in a Kubernetes cluster has the required services to be managed by the master. Nodes also have the required services to run pods, including the container runtime, a kubelet, and a service proxy. OpenShift Container Platform creates nodes from a cloud provider, physical systems, or virtual systems. Kubernetes interacts with node objects that are a representation of those nodes. The master uses the information from node objects to validate nodes with health checks. A node is ignored until it passes the health checks, and the master continues checking nodes until they are valid. We can manage nodes in our instance using the CLI. When we perform node management operations, the CLI interacts with node objects that are representations of actual node hosts. The master uses the information from node objects to validate nodes with health checks. You can review cluster node health status, resource consumption statistics, and node logs using the below commands. Additionally, you can query kubelet status on individual nodes.","title":"Managing Node Health"},{"location":"Node_Health_Management/#1-node-reporting-notready","text":"The mentioned problem was seen on 4.5.4 running on KVM/libvirt, but this could happen and be applied on other platforms. Some nodes in this 3-master/5-worker cluster start showing \"NotReady\" state with \"Kubelet stopped posting node status\" in oc describe node output. SSH'ing into those \"NotReady\" nodes, it could be seen that Current certificate is expired in journalctl -u kubelet , and that is precisely the issue preventing communication with API server. You might also see: Failed Units: 1 NetworkManager-wait-online.service [core@worker-0 ~]$ upon SSH login, but that is a red herring. In order to resolve this issue run the following commands on \"NotReady\" node(s): $ sudo systemctl restart kubelet followed by, anywhere you can run oc commands against the cluster; $ oc get csr | grep Pending | awk '{print $1}' | xargs oc adm certificate approve Note - Make sure you have no more \"Pending\" CSRs after a minute or two as well.","title":"1. Node reporting NotReady"},{"location":"Node_Health_Management/#2-querying-the-kubelets-status-on-a-node","text":"The kubelet is managed using a systemd service on each node. Review the kubelet\u2019s status by querying the kubelet systemd service within a debug Pod. a. Start a debug Pod for a node: $ oc debug node/node_name b. Set /host as the root directory within the debug shell. The debug Pod mounts the host\u2019s root file system in /host within the Pod. By changing the root directory to /host, you can run binaries contained in the host\u2019s executable paths: # chroot /host c. Check whether the kubelet systemd service is active on the node: # systemctl is-active kubelet d. Output a more detailed kubelet.service status summary: # systemctl status kubelet Status on CLI oc debug node/master0.ocp-support-suad.cp.fyre.ibm.com Starting pod/master0ocp-support-suadcpfyreibmcom-debug ... To use host binaries, run `chroot /host` Pod IP: 10.17.73.96 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# systemctl is-active kubelet active sh-4.4# systemctl status kubelet \u25cf kubelet.service - MCO environment configuration Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d \u2514\u250010-mco-default-env.conf Active: active (running) since Thu 2020-08-27 07:47:12 UTC; 1 day 3h ago Process: 1347 ExecStartPre=/bin/rm -f /var/lib/kubelet/cpu_manager_state (code=exited, status=0/SUCCESS) Process: 1344 ExecStartPre=/bin/mkdir --parents /etc/kubernetes/manifests (code=exited, status=0/SUCCESS) Main PID: 1349 (kubelet) Tasks: 38 (limit: 51304) Memory: 264.3M CPU: 7h 15min 31.395s CGroup: /system.slice/kubelet.service \u2514\u25001349 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-en> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827568 1349 atomic_writer.go:157] pod openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-769d7b> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827609 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"config\" (UniqueName: \"kubernetes> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827720 1349 secret.go:183] Setting up volume node-ca-token-r9zm6 for pod 42e3fbda-1361-4c75-9f72-7938a75a6073 at> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.827792 1349 secret.go:207] Received secret openshift-image-registry/node-ca-token-r9zm6 containing (4) pieces of> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828220 1349 atomic_writer.go:157] pod openshift-image-registry/node-ca-vhm68 volume node-ca-token-r9zm6: no upda> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828265 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"node-ca-token-r9zm6\" (UniqueName> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828357 1349 secret.go:183] Setting up volume serving-cert for pod a3679c68-645c-4b58-9324-b03a923624f0 at /var/l> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828419 1349 secret.go:207] Received secret openshift-kube-scheduler-operator/kube-scheduler-operator-serving-cer> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828681 1349 atomic_writer.go:157] pod openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-769d7b> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.828717 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"serving-cert\" (UniqueName: \"kube> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.837829 1349 atomic_writer.go:157] pod openshift-image-registry/node-ca-vhm68 volume serviceca: no update require> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.837926 1349 operation_generator.go:657] MountVolume.SetUp succeeded for volume \"serviceca\" (UniqueName: \"kuberne> Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.970203 1349 exec.go:60] Exec probe response: \"\" Aug 28 11:35:41 master0.ocp-support-suad.cp.fyre.ibm.com hyperkube[1349]: I0828 11:35:41.970272 1349 prober.go:133] Liveness probe for \"ovs-p2qkz_openshift-sdn(6d62e5db-053b-4573-95ce-d7d478f578d9):ope>","title":"2. Querying the kubelet\u2019s status on a node"},{"location":"Node_Health_Management/#3-node-logs-get-logs-for-networkmanager","text":"The below command will give the logs of the network manager of a particular node of a cluster. $ oc adm node-logs --role master -u NetworkManager.service Sep 06 04:01:37.044137 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0439] dhcp4 (enp0s3): option dhcp_lease_time => '600' Sep 06 04:01:37.044177 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0441] dhcp4 (enp0s3): option domain_name => 'cp.fyre.ibm.com' Sep 06 04:01:37.044199 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0441] dhcp4 (enp0s3): option domain_name_servers => '10.17.64.21 10.17.64.22' Sep 06 04:01:37.044219 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option domain_search => 'cp.fyre.ibm.com' Sep 06 04:01:37.044239 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option expiry => '1599365497' Sep 06 04:01:37.044259 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option ip_address => '10.17.73.158' Sep 06 04:01:37.044280 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option next_server => '10.17.64.9' Sep 06 04:01:37.044301 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0442] dhcp4 (enp0s3): option requested_broadcast_address => '1' Sep 06 04:01:37.044322 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_domain_name => '1' Sep 06 04:01:37.044342 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_domain_name_servers => '1' Sep 06 04:01:37.044363 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_domain_search => '1' Sep 06 04:01:37.044383 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_host_name => '1' Sep 06 04:01:37.044403 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0443] dhcp4 (enp0s3): option requested_interface_mtu => '1' Sep 06 04:01:37.044424 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_ms_classless_static_routes => '1' Sep 06 04:01:37.044471 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_nis_domain => '1' Sep 06 04:01:37.044491 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_nis_servers => '1' Sep 06 04:01:37.044511 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0444] dhcp4 (enp0s3): option requested_ntp_servers => '1' Sep 06 04:01:37.044532 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_rfc3442_classless_static_routes => '1' Sep 06 04:01:37.044552 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_root_path => '1' Sep 06 04:01:37.044572 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_routers => '1' Sep 06 04:01:37.044593 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_static_routes => '1' Sep 06 04:01:37.044613 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0445] dhcp4 (enp0s3): option requested_subnet_mask => '1' Sep 06 04:01:37.044637 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0446] dhcp4 (enp0s3): option requested_time_offset => '1' Sep 06 04:01:37.044658 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0446] dhcp4 (enp0s3): option requested_wpad => '1' Sep 06 04:01:37.044734 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0446] dhcp4 (enp0s3): option routers => '10.17.64.1' Sep 06 04:01:37.044759 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0447] dhcp4 (enp0s3): option subnet_mask => '255.255.224.0' Sep 06 04:01:37.044779 master2.ocp-support-suad.cp.fyre.ibm.com NetworkManager[1163]: <info> [1599364897.0447] dhcp4 (enp0s3): state changed extended -> extended","title":"3. Node-logs: Get logs for NetworkManager"},{"location":"Node_Health_Management/#4-nodes-report-ready-but-etcd-health-check-fails","text":"[root@master02 ~]# etcdctl -C https://master00.ose.example.com:2379,https://master01.ose.example.com:2379,https://master01.ose.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health member e0e2c123213680f is healthy: got healthy result from https://192.168.200.50:2379 member 64f1077d838e039c is healthy: got healthy result from https://192.168.200.51:2379 member a9e031ea9ce2a521 is unhealthy: got unhealthy result from https://192.168.200.52:2379 In the event that the health check fails check the status of etcd you could see one or a combination of the following: [root@master02 ~]# etcdctl -C https://master00.ose.example.com:2379,https://master01.ose.example.com:2379,https://master01.ose.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health member e0e2c123213680f is healthy: got healthy result from https://192.168.200.50:2379 member 64f1077d838e039c is healthy: got healthy result from https://192.168.200.51:2379 member a9e031ea9ce2a521 is unhealthy: got unhealthy result from https://192.168.200.52:2379 [root@master01 ~]# systemctl status etcd \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2016-02-25 08:43:37 CST; 4h 32min ago Main PID: 1103 (etcd) CGroup: /system.slice/etcd.service \u2514\u25001103 /usr/bin/etcd --name=master01.ose.example.com --data-dir=/var/lib/etcd/ --lis... Feb 25 11:32:52 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:32:52 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:02 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:02 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:12 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) Feb 25 11:33:12 master01 etcd[1103]: got unexpected response error (etcdserver: request timed out) [root@master00 ~]# systemctl status etcd \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2016-02-25 08:43:55 CST; 4h 32min ago Main PID: 1097 (etcd) CGroup: /system.slice/etcd.service \u2514\u25001097 /usr/bin/etcd --name=master00.ose.example.com --data-dir=/var/lib/etcd/ --lis... Feb 25 11:40:25 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:40:55 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:41:25 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:41:55 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Feb 25 11:42:25 master00 etcd[1097]: the connection to peer a9e031ea9ce2a521 is unhealthy Solution In most cases restarting etcd one at a time on each etcd host resolves the issue $ sudo systemctl restart etcd Reference Link - http://v1.uncontained.io/playbooks/troubleshooting/troubleshooting_guide.html","title":"4. Nodes report ready but ETCD health check fails"},{"location":"Node_Health_Management/#5-file-system-trackinga-complete-monitoring-for-disk-usage","text":"To monitor filesystems and more precisely the overall space remaining on various filesystems , the node exporter exports two metrics to retrieve such statistics : a. node_filesystem_avail_bytes b. node_filesystem_size_bytes Use below query in Prometheus for the overall filesystem usage by device or by mountpoint : ( 1 - node_filesystem_avail_bytes /node_filesystem_size_bytes )*100","title":"5. File System tracking(a complete monitoring for disk usage)"},{"location":"Node_Health_Management/#6-read-write-latencies","text":"Another great metric for monitoring is the read and write latencies on our disks. The node exporter exports multiple metrics to compute it. On the read side, we have: a. node_disk_read_time_seconds_total b. node_disk_reads_completed_total On the write side, we have: a. node_disk_write_time_seconds_total b. node_disk_writes_completed_total If we compute rates for the two metrics, and divide one by the other, we are able to compute the latency or the time that your disk takes in order to complete such operations. Use below query in Prometheus for the disk latency : rate(node_disk_write_time_seconds_total[5s])/rate(node_disk_writes_completed_total[5s])*100","title":"6. Read &amp; Write Latencies"},{"location":"Prometheus_Queries/","text":"PROMETHEUS QUERIES FOR MONITORING OF OC CLUSTER Prometheus provides a functional query language called PromQL (Prometheus Query Language) that lets the user select and aggregate time series data in real time. The result of an expression can either be shown as a graph, viewed as tabular data in Prometheus's expression browser, or consumed by external systems via the HTTP API.This document discusses about some Prometheus Queries which might help the users for monitoring and tracking their clusters. 1. Overall I/O load on your instance The below expression is used for the global health-check of our system, we will be able to know the overall I/O load on our system. The node exporter exposes the following metric : rate(node_disk_io_now[5s]) and computing the rate for this metric will give the overall I/O load. Here, [5s] represents the time duration d of the query to be executed. Higher d smooths the graph, while lower d makes the graph more precise. 2. Node disk's I/O Queries Below are the various I/O queries made on the node disks: node_disk_io_time_seconds_total -- It serves as a counter and is incremented when I/O is in progress and hence is used to calculate disk I/O utilisation. node_disk_read_bytes_total -- It gives the total bytes read by the I/Os. node_disk_written_bytes_total -- It shows the number of bytes written by the I/Os. node_disk_read_time_seconds_total -- It shows the total time taken by a read I/Os. node_disk_write_time_seconds_total -- This is the total number of seconds spent by all writes. node_disk_reads_completed_total -- It shows the number of read I/Os that are completed. node_disk_writes_completed_total -- The total number of writes completed successfully. 3. Comparing current data with historical data PromQL allows querying historical data and combining / comparing it to the current data. Just add offset to the query. For instance, the following query would return week-old data for all the time series with node_network_receive_bytes_total name: node_network_receive_bytes_total offset 7d 4. etcd Monitoring etcd is an open source distributed key-value store used to hold and manage the critical information that distributed systems need to keep running. Most notably, it manages the configuration data, state data, and metadata for Kubernetes. If the etcd service does not run correctly, successful operation of the whole OpenShift Container Platform cluster is in danger. Therefore, it is reasonable to configure monitoring of etcd. Some of the metrics that are available and useful in our experience are as follows: etcd_cluster_version (gauge) : It shows which version is running. etcd_debugging_disk_backend_commit_rebalance_duration_seconds (cumulative) : The latency distributions of commit.rebalance called by bboltdb backend. (sum) etcd_debugging_lease_revoked_total (cumulative) : The total number of revoked leases. etcd_debugging_lease_ttl_total (cumulative) : Bucketed histogram of lease TTLs. (sum) etcd_debugging_mvcc_db_total_size_in_bytes (gauge) : Total size of the underlying database physically allocated in bytes. etcd_debugging_mvcc_delete_total (cumulative) : Total number of deletes seen by this member. 5. Resource Metric: Kubernetes CPU Usage The below expression will give the CPU usage for all pods belonging to the cluster. sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"})) 6. Resource Metric: Kubernetes Container CPU usage The first expression will show CPU usage for individual instances of each container whereas the second expression aggregates CPU usage for all containers with the same name. a. rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"}) b. sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"})) by (container_name)","title":"PROMETHEUS QUERIES FOR MONITORING OF OC CLUSTER"},{"location":"Prometheus_Queries/#prometheus-queries-for-monitoring-of-oc-cluster","text":"Prometheus provides a functional query language called PromQL (Prometheus Query Language) that lets the user select and aggregate time series data in real time. The result of an expression can either be shown as a graph, viewed as tabular data in Prometheus's expression browser, or consumed by external systems via the HTTP API.This document discusses about some Prometheus Queries which might help the users for monitoring and tracking their clusters.","title":"PROMETHEUS QUERIES FOR MONITORING OF OC CLUSTER"},{"location":"Prometheus_Queries/#1-overall-io-load-on-your-instance","text":"The below expression is used for the global health-check of our system, we will be able to know the overall I/O load on our system. The node exporter exposes the following metric : rate(node_disk_io_now[5s]) and computing the rate for this metric will give the overall I/O load. Here, [5s] represents the time duration d of the query to be executed. Higher d smooths the graph, while lower d makes the graph more precise.","title":"1. Overall I/O load on your instance"},{"location":"Prometheus_Queries/#2-node-disks-io-queries","text":"Below are the various I/O queries made on the node disks: node_disk_io_time_seconds_total -- It serves as a counter and is incremented when I/O is in progress and hence is used to calculate disk I/O utilisation. node_disk_read_bytes_total -- It gives the total bytes read by the I/Os. node_disk_written_bytes_total -- It shows the number of bytes written by the I/Os. node_disk_read_time_seconds_total -- It shows the total time taken by a read I/Os. node_disk_write_time_seconds_total -- This is the total number of seconds spent by all writes. node_disk_reads_completed_total -- It shows the number of read I/Os that are completed. node_disk_writes_completed_total -- The total number of writes completed successfully.","title":"2. Node disk's I/O Queries"},{"location":"Prometheus_Queries/#3-comparing-current-data-with-historical-data","text":"PromQL allows querying historical data and combining / comparing it to the current data. Just add offset to the query. For instance, the following query would return week-old data for all the time series with node_network_receive_bytes_total name: node_network_receive_bytes_total offset 7d","title":"3. Comparing current data with historical data"},{"location":"Prometheus_Queries/#4-etcd-monitoring","text":"etcd is an open source distributed key-value store used to hold and manage the critical information that distributed systems need to keep running. Most notably, it manages the configuration data, state data, and metadata for Kubernetes. If the etcd service does not run correctly, successful operation of the whole OpenShift Container Platform cluster is in danger. Therefore, it is reasonable to configure monitoring of etcd. Some of the metrics that are available and useful in our experience are as follows: etcd_cluster_version (gauge) : It shows which version is running. etcd_debugging_disk_backend_commit_rebalance_duration_seconds (cumulative) : The latency distributions of commit.rebalance called by bboltdb backend. (sum) etcd_debugging_lease_revoked_total (cumulative) : The total number of revoked leases. etcd_debugging_lease_ttl_total (cumulative) : Bucketed histogram of lease TTLs. (sum) etcd_debugging_mvcc_db_total_size_in_bytes (gauge) : Total size of the underlying database physically allocated in bytes. etcd_debugging_mvcc_delete_total (cumulative) : Total number of deletes seen by this member.","title":"4. etcd Monitoring"},{"location":"Prometheus_Queries/#5-resource-metric-kubernetes-cpu-usage","text":"The below expression will give the CPU usage for all pods belonging to the cluster. sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"}))","title":"5. Resource Metric: Kubernetes CPU Usage"},{"location":"Prometheus_Queries/#6-resource-metric-kubernetes-container-cpu-usage","text":"The first expression will show CPU usage for individual instances of each container whereas the second expression aggregates CPU usage for all containers with the same name. a. rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"}) b. sum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",pod_name!=\"\"})) by (container_name)","title":"6. Resource Metric: Kubernetes Container CPU usage"},{"location":"actually-helped/","text":"Collection of issues we faced and external pages/links that actually helped resolve them; 2021-02 image-registry operator reporting Progressing, while Available and not Degraded, after starting with \"EmptyDir\" storage, switching to PVC (with nfs-provisioner ), then deleting and recreating the provisioner Pod. The cluster config of image-registry ends up with 2 storage types, which it cannot support at this time (tested/verified with 4.6.13). To edit the config, use: $ oc edit configs.imageregistry.operator.openshift.io => https://access.redhat.com/solutions/4516391 2021-01 4.6 node reporting NotReady after upgrade to 4.6.12, due to CRI-O failing to start up on one worker node (other nodes were fine). The error seen in journalctl on the troubling node was like Jan 21 16:00:48 worker1.ocp4.example.com bash[1881]: Error: readlink /var/lib/containers/storage/overlay/l/6MDZKPV63T> Running # systemctl daemon-reload was necessary before the storage wipe steps in the below doc. => https://access.redhat.com/solutions/5350721 (again, see 2020-10 ) OCS (OpenShift Container Storage) 4.6 install worked like a champ following this document. => https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.6/html-single/monitoring_openshift_container_storage/index (2020-10-26 4.6 GA) 2020-10 Pods failing to start on a particular node, often with ImagePullBackOff error, due to getting CRI-O ephemeral storage full => https://access.redhat.com/solutions/5350721 How to troubleshoot ingress, routes, services, what objects to check, in what order (perhaps for likeliness/quicker resolution) => https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/ 4.5 disconnected cluster (ie. no direct access to Internet), normal upgrade fails, requiring \"opting out Insights operator\" and \"mirroring image repository (again).\" oc adm upgrade first checks if the cluster is currently healthy, and any problem with cluster operators ( oc get co ) blocks upgrade operation from proceeding. The Insights operator would be running but degraded because it cannot report status to Red Hat due to no Internet access, hence needs \"opting out.\" Then, the upgrade-to version images need to be mirrored to the internal repository, again, due to no Internet access. => https://docs.openshift.com/container-platform/4.5/support/remote_health_monitoring/opting-out-of-remote-health-reporting.html => https://docs.openshift.com/container-platform/4.5/updating/updating-restricted-network-cluster.html How to install, configure, use Node Feature Discovery operator. The article was written for OCP 4.1, therefore the UI is a little different in 4.6, but it's actually easier (without having to deal with YAML) in 4.6. => https://access.redhat.com/solutions/4734811 2020-09 4.5 on libvirt, authentication and console operators not available, and console pod's log indicating DNS issue for \"oauth-openshift. \" => https://github.com/openshift/installer/issues/1648#issuecomment-585235423 2020-08 node NotReady after reboot perhaps after shut down for a few days, with \"Kubelet stopped posting node status\" in oc describe node essentially, sudo systemctl restart kubelet on the bad node, followed by oc get csr | grep Pending | awk '{print $1}' | xargs oc adm certificate approve , perhaps multiple times as a few CSRs could come in for approval sequentially. => https://docs.openshift.com/container-platform/4.1/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html random intermittent API server errors like \"connection refused,\" \"authentication required,\" \"internal error,\" with etcd running on slow disk => https://github.com/openshift/release/blob/23074b5/ci-operator/templates/openshift/installer/cluster-launch-installer-openstack-e2e.yaml#L375-L382 (2020-07-30 4.5 GA) (2020-06-23 4.4 GA) 2020-06 OOMKilled on large POWER system, while same set of pods run fine on smaller x86 system, with possible tinkering with slub_max_order kernel param => https://medium.com/ibm-cloud/fortifying-ibm-cloud-private-for-large-enterprise-power-systems-6119804f0103 (2020-04-30 4.3 GA) 2020-02 4.x image registry not accessible externally by default, need default-route created => https://docs.openshift.com/container-platform/4.3/registry/securing-exposing-registry.html => https://docs.openshift.com/container-platform/4.4/registry/configuring-registry-operator.html#registry-operator-default-crd_configuring-registry-operator","title":"External"},{"location":"actually-helped/#2021-02","text":"image-registry operator reporting Progressing, while Available and not Degraded, after starting with \"EmptyDir\" storage, switching to PVC (with nfs-provisioner ), then deleting and recreating the provisioner Pod. The cluster config of image-registry ends up with 2 storage types, which it cannot support at this time (tested/verified with 4.6.13). To edit the config, use: $ oc edit configs.imageregistry.operator.openshift.io => https://access.redhat.com/solutions/4516391","title":"2021-02"},{"location":"actually-helped/#2021-01","text":"4.6 node reporting NotReady after upgrade to 4.6.12, due to CRI-O failing to start up on one worker node (other nodes were fine). The error seen in journalctl on the troubling node was like Jan 21 16:00:48 worker1.ocp4.example.com bash[1881]: Error: readlink /var/lib/containers/storage/overlay/l/6MDZKPV63T> Running # systemctl daemon-reload was necessary before the storage wipe steps in the below doc. => https://access.redhat.com/solutions/5350721 (again, see 2020-10 ) OCS (OpenShift Container Storage) 4.6 install worked like a champ following this document. => https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.6/html-single/monitoring_openshift_container_storage/index","title":"2021-01"},{"location":"actually-helped/#2020-10-26-46-ga","text":"","title":"(2020-10-26 4.6 GA)"},{"location":"actually-helped/#2020-10","text":"Pods failing to start on a particular node, often with ImagePullBackOff error, due to getting CRI-O ephemeral storage full => https://access.redhat.com/solutions/5350721 How to troubleshoot ingress, routes, services, what objects to check, in what order (perhaps for likeliness/quicker resolution) => https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/ 4.5 disconnected cluster (ie. no direct access to Internet), normal upgrade fails, requiring \"opting out Insights operator\" and \"mirroring image repository (again).\" oc adm upgrade first checks if the cluster is currently healthy, and any problem with cluster operators ( oc get co ) blocks upgrade operation from proceeding. The Insights operator would be running but degraded because it cannot report status to Red Hat due to no Internet access, hence needs \"opting out.\" Then, the upgrade-to version images need to be mirrored to the internal repository, again, due to no Internet access. => https://docs.openshift.com/container-platform/4.5/support/remote_health_monitoring/opting-out-of-remote-health-reporting.html => https://docs.openshift.com/container-platform/4.5/updating/updating-restricted-network-cluster.html How to install, configure, use Node Feature Discovery operator. The article was written for OCP 4.1, therefore the UI is a little different in 4.6, but it's actually easier (without having to deal with YAML) in 4.6. => https://access.redhat.com/solutions/4734811","title":"2020-10"},{"location":"actually-helped/#2020-09","text":"4.5 on libvirt, authentication and console operators not available, and console pod's log indicating DNS issue for \"oauth-openshift. \" => https://github.com/openshift/installer/issues/1648#issuecomment-585235423","title":"2020-09"},{"location":"actually-helped/#2020-08","text":"node NotReady after reboot perhaps after shut down for a few days, with \"Kubelet stopped posting node status\" in oc describe node essentially, sudo systemctl restart kubelet on the bad node, followed by oc get csr | grep Pending | awk '{print $1}' | xargs oc adm certificate approve , perhaps multiple times as a few CSRs could come in for approval sequentially. => https://docs.openshift.com/container-platform/4.1/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html random intermittent API server errors like \"connection refused,\" \"authentication required,\" \"internal error,\" with etcd running on slow disk => https://github.com/openshift/release/blob/23074b5/ci-operator/templates/openshift/installer/cluster-launch-installer-openstack-e2e.yaml#L375-L382","title":"2020-08"},{"location":"actually-helped/#2020-07-30-45-ga","text":"","title":"(2020-07-30 4.5 GA)"},{"location":"actually-helped/#2020-06-23-44-ga","text":"","title":"(2020-06-23 4.4 GA)"},{"location":"actually-helped/#2020-06","text":"OOMKilled on large POWER system, while same set of pods run fine on smaller x86 system, with possible tinkering with slub_max_order kernel param => https://medium.com/ibm-cloud/fortifying-ibm-cloud-private-for-large-enterprise-power-systems-6119804f0103","title":"2020-06"},{"location":"actually-helped/#2020-04-30-43-ga","text":"","title":"(2020-04-30 4.3 GA)"},{"location":"actually-helped/#2020-02","text":"4.x image registry not accessible externally by default, need default-route created => https://docs.openshift.com/container-platform/4.3/registry/securing-exposing-registry.html => https://docs.openshift.com/container-platform/4.4/registry/configuring-registry-operator.html#registry-operator-default-crd_configuring-registry-operator","title":"2020-02"},{"location":"contrib/","text":"Basic flow of working on the contents Adding/Editing Contents Fork https://github.com/ocp-power-automation/extra perhaps naming your fork \"power-automation-extra\" as an example git clone https://github.com/<yourid>/power-automation-extra.git cd power-automation-extra pip install mkdocs mkdocs serve - this will start the live-reloading docs server on port 8000, ctrl-c to stop edit docs under the ./docs subdir with your favorite editor (1 sentence per line) point your browser to http://localhost:8000 repeat the edit & check until you are happy git add your changes git commit -s with good comment git push to your forked repo Create a PR against upstream master branch Reviewing Changes New (pre-merge) PR submitted by somebody (eg. \"pull/27\") Let's say you are in the power-automation-extra directory where origin points to your repo git remote add upstream https://github.com/ocp-power-automation/extra.git (if you have not done this already) git fetch upstream pull/27/head:review27 - this creates a remote branch called \"review27\" in your repo with the PR git checkout review27 - this downloads the source tree with the PR mkdocs serve point your browser to http://localhost:8000 Command References git git-add git-checkout git-clone git-commit git-fetch git-push git-remote mkdocs pip","title":"Contribute"},{"location":"contrib/#addingediting-contents","text":"Fork https://github.com/ocp-power-automation/extra perhaps naming your fork \"power-automation-extra\" as an example git clone https://github.com/<yourid>/power-automation-extra.git cd power-automation-extra pip install mkdocs mkdocs serve - this will start the live-reloading docs server on port 8000, ctrl-c to stop edit docs under the ./docs subdir with your favorite editor (1 sentence per line) point your browser to http://localhost:8000 repeat the edit & check until you are happy git add your changes git commit -s with good comment git push to your forked repo Create a PR against upstream master branch","title":"Adding/Editing Contents"},{"location":"contrib/#reviewing-changes","text":"New (pre-merge) PR submitted by somebody (eg. \"pull/27\") Let's say you are in the power-automation-extra directory where origin points to your repo git remote add upstream https://github.com/ocp-power-automation/extra.git (if you have not done this already) git fetch upstream pull/27/head:review27 - this creates a remote branch called \"review27\" in your repo with the PR git checkout review27 - this downloads the source tree with the PR mkdocs serve point your browser to http://localhost:8000","title":"Reviewing Changes"},{"location":"contrib/#command-references","text":"git git-add git-checkout git-clone git-commit git-fetch git-push git-remote mkdocs pip","title":"Command References"},{"location":"h2t-pending-pods/","text":"How to Troubleshoot Pending Pods ======================================================================= A pod stuck in pending state cannot be scheduled onto a node until the problem is identified and resolved. It could be caused due to a handful of reasons ranging from resource issues to the rules applied to the pods and nodes. For this, a few commands could be executed to narrow down the possibilities and focus on what could have been the exact issue causing it. A few initial diagnostic commands mentioned below could be a starting step to help troubleshoot the pods stuck in pending state. Initial diagnosis: oc adm top nodes This command will show how CPU and Memory are tight across nodes in the cluster. If you see high percentage number(s), the node(s) is/are running out of that resource, and possibly preventing new pods from being scheduled and starting. Check reason 1 below. oc describe node node-name This is a good follow-up command to above, and will provide detailed information of a specific node. Checking the information under 'Allocated resources' or 'Conditions' could help in understanding if the problem is inclining towards insufficient resources(memory or CPU) on the node and in determining the resources to be allocated to the pod accordingly oc get pods -A | grep Pending This command will show pods that are in Pending state in the cluster. If you see only a handful, you are in luck and proceed with below oc describe pod command for those to find out more details and which subsection of this document to go. If you see many more Pending pods, it may be quicker to suspect more systemic issues, than individual pod issues, although it may still come down to it. oc describe pod pod-name The 'Events' section of the output would provide the current status of the pod and also the reason for it being in that state. If it is stuck in pending state, based on the reason, that particular issue can be resolved by following the steps below 1. Reason: Insufficient resources on the nodes available Error ==> `0/5 nodes are available: 1 Insufficient cpu` Pod went to pending state if you don't have enough resources: You may have exhausted the supply of CPU or Memory in your cluster. How can this be resolved? Viewing the CPU and memory usage statistics on the nodes could help in better allocation of a pod to the desired node. This can be checked by oc adm top nodes Note: permission is required to view the usage statistics Here are some example command lines that extract just the necessary information. oc get nodes -o yaml | egrep '\\sname:|cpu:|memory:' name: master-0.202-8880.example.com cpu: 7500m memory: 16101184Ki cpu: \"8\" memory: 16715584Ki oc get nodes -o json | jq '.items[] | {name: .metadata.name, cap: .status.capacity}' { \"name\": \"master-0.202-8880.example.com\", \"cap\": { \"cpu\": \"8\", \"ephemeral-storage\": \"209290220Ki\", \"hugepages-16Gi\": \"0\", \"hugepages-16Mi\": \"0\", \"memory\": \"16715584Ki\", \"pods\": \"250\" } } To view the statistics of a node based labels - (Operators used: =, != and ==). Note: permission is required to view the usage statistics. To delete all pods which are completed/failed and free the resources use the below command. For a current namespace $(oc get pods | grep Error | awk '{print $1}'); do oc delete pod --grace-period=1 ${pod}; done For all namespaces $(oc get pods --all-namespaces | grep Error | awk '{print $1}'); do oc delete pod --grace period=1 ${pod}; done 2. Reason: Node and pod affinity rules applied to the pod Error ==> 1 node(s) didn't match pod affinity/anti-affinity If you do not carefully configure pod affinity, node affinity and pod anti-affinity with equal-priority pods, the pod might not be scheduled and ends up in pending/waiting state Depending on your pod priority and preemption settings, the scheduler might not be able to find an appropriate node for a pod without violating affinity requirements. How can this be resolved? Pod affinity, pod anti-affinity and node affinity allow you to constrain which nodes your pod is eligible to be scheduled on based on the key/value labels on other pods. Here is a sample configuration to help understand the application of affinity rules to nodes and pods Sample pod configuration with a node selector rule : spec: nodeSelector: beta.kubernetes.io/os: linux node-role.kubernetes.io/worker: '' type: user-node Sample pod configuration with a node affinity preferred rule : spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: e2e-az-EastWest operator: In values: - e2e-az-East - e2e-az-West Using node selectors and node affinity in the same pod configuration, note the following: If you configure both nodeSelector and nodeAffinity, both conditions must be satisfied for the pod to be scheduled onto a candidate node. If you specify multiple nodeSelectorTerms associated with nodeAffinity types, then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied. If you specify multiple matchExpressions associated with nodeSelectorTerms, then the pod can be scheduled onto a node only if all matchExpressions are satisfied. 3. Reason: Applying taints and tolerations Error ==> 1 node(s) had taints that the pod didn't tolerate When a taint is applied to a node, it repels the scheduling of a pod on that node unless it matches the taint with its tolerations. How can this be resolved? Ensure that the tolerations are applied to the pods such that they match the taints of the nodes to end up being scheduled on the desired one. The taints applied to a node can be checked through oc describe node node-name | grep -i taint . The pod toleration in the respective yaml file can be verified to check if the pod can be scheduled on that particular node. Example : If a particular node has the following taints: oc describe node node-1 | grep -i taint Taints: node.alpha.kubernetes.io/not-ready:NoExecute The pod with the appropriate tolerations would be schedules on node-1 such as: tolerations: - key: \"node.alpha.kubernetes.io/not-ready\" operator: \"Exists\" effect: \"NoExecute\" 4. Reason: Issues with Container engine on the host Error ==> failed to pull image The specific image being used by the pod might not be available in the repository it is being pulled from. As an example Normal Pulling 10s (x4 over 94s) kubelet, worker-1.prashanth-3d14.redhat.com Pulling image \"hello-openshift\" Warning Failed 9s (x4 over 93s) kubelet, worker-1.prashanth-3d14.redhat.com Failed to pull image \"hello-openshift\": rpc error: code = Unknown desc = Error reading manifest latest in docker.io/hello-openshift/hello: errors How can this be resolved? The following can be checked: Make sure that you have the name of the image correct. Does the image exist in the repository being pulled from? Have you checked if the image is pushed to the repository? Run a manual podman/docker pull image on your machine to see if the image can be pulled. 5. Reason: PersistentVolumeClaims Error ==> pod has unbound immediate PersistentVolumeClaims Pod can get stuck in pending state because the pod may have unbound immediate PersistentVolumeClaims. Can be checked with command - oc describe pod podname and oc get pvc -n namespace shows status of pvc in unbound state. As an example. Warning FailedScheduling 22s (x2 over 22s) default-scheduler pod has unbound immediate PersistentVolumeClaims How can this be resolved? The OCP cluster inspects the persistent volume claim to find the bound volume and mounts that volume for a Pod. For those persistent volumes that support multiple access modes, you must specify which mode applies when you use the claim as a volume in a Pod, in other words the 'accesModes' mentioned in the persistent volume claim of a given pod must match any existing persistent volume for the pod to be scheduled. The claims remain unbound indefinitely if a matching volume does not exist or cannot be created with any available provisioner servicing a storage class. Consider the following PV to be existing on the cluster. Persitent volume specification: apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain For a pod to be scheduled on a given node in the cluster, the PVC should consist of the following specifications. Persistent Volume Claim specifications for a pod: kind: PersistentVolumeClaim metadata: name: nfs-claim1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi 6. Reason: Using host port Error ==> node(s) didn't have free' ports for the requested pod ports When a pod uses a 'hostPort' for any of its containers, there are a limited number of places that the pod can be scheduled. The following error would be faced. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 6s (x25 over 34s) default-scheduler 0/9 nodes are available: 4 node(s) didn't have free ports for the requested pod ports, 7 node(s) didn't match node selector. How can this be resolved? The below command outputs the container ports in all namespaces. oc get po --all-namespaces -o=jsonpath=\"{range .items[*]}{.spec.nodeName}{'\\t'}{.spec.hostNetwork}{'\\t'}{.spec.hostNetwork}{'\\t'}{.spec.containers..containerPort}{'\\n'}{end}\" Using the above command, the occupied ports for the exisiting containers on all the nodes can be checked and if the port being requested is already occupied, the hostPort for the particular container can be modified in the requested pod configuration to use another port.","title":"How to Troubleshoot Pending Pods"},{"location":"h2t-pending-pods/#how-to-troubleshoot-pending-pods","text":"======================================================================= A pod stuck in pending state cannot be scheduled onto a node until the problem is identified and resolved. It could be caused due to a handful of reasons ranging from resource issues to the rules applied to the pods and nodes. For this, a few commands could be executed to narrow down the possibilities and focus on what could have been the exact issue causing it. A few initial diagnostic commands mentioned below could be a starting step to help troubleshoot the pods stuck in pending state.","title":"How to Troubleshoot Pending Pods"},{"location":"h2t-pending-pods/#initial-diagnosis","text":"oc adm top nodes This command will show how CPU and Memory are tight across nodes in the cluster. If you see high percentage number(s), the node(s) is/are running out of that resource, and possibly preventing new pods from being scheduled and starting. Check reason 1 below. oc describe node node-name This is a good follow-up command to above, and will provide detailed information of a specific node. Checking the information under 'Allocated resources' or 'Conditions' could help in understanding if the problem is inclining towards insufficient resources(memory or CPU) on the node and in determining the resources to be allocated to the pod accordingly oc get pods -A | grep Pending This command will show pods that are in Pending state in the cluster. If you see only a handful, you are in luck and proceed with below oc describe pod command for those to find out more details and which subsection of this document to go. If you see many more Pending pods, it may be quicker to suspect more systemic issues, than individual pod issues, although it may still come down to it. oc describe pod pod-name The 'Events' section of the output would provide the current status of the pod and also the reason for it being in that state. If it is stuck in pending state, based on the reason, that particular issue can be resolved by following the steps below","title":"Initial diagnosis:"},{"location":"h2t-pending-pods/#1-reason-insufficient-resources-on-the-nodes-available","text":"Error ==> `0/5 nodes are available: 1 Insufficient cpu` Pod went to pending state if you don't have enough resources: You may have exhausted the supply of CPU or Memory in your cluster.","title":"1. Reason:\u00a0Insufficient resources on the nodes available"},{"location":"h2t-pending-pods/#how-can-this-be-resolved","text":"Viewing the CPU and memory usage statistics on the nodes could help in better allocation of a pod to the desired node. This can be checked by oc adm top nodes Note: permission is required to view the usage statistics Here are some example command lines that extract just the necessary information. oc get nodes -o yaml | egrep '\\sname:|cpu:|memory:' name: master-0.202-8880.example.com cpu: 7500m memory: 16101184Ki cpu: \"8\" memory: 16715584Ki oc get nodes -o json | jq '.items[] | {name: .metadata.name, cap: .status.capacity}' { \"name\": \"master-0.202-8880.example.com\", \"cap\": { \"cpu\": \"8\", \"ephemeral-storage\": \"209290220Ki\", \"hugepages-16Gi\": \"0\", \"hugepages-16Mi\": \"0\", \"memory\": \"16715584Ki\", \"pods\": \"250\" } } To view the statistics of a node based labels - (Operators used: =, != and ==). Note: permission is required to view the usage statistics. To delete all pods which are completed/failed and free the resources use the below command. For a current namespace $(oc get pods | grep Error | awk '{print $1}'); do oc delete pod --grace-period=1 ${pod}; done For all namespaces $(oc get pods --all-namespaces | grep Error | awk '{print $1}'); do oc delete pod --grace period=1 ${pod}; done","title":"How can this be resolved?"},{"location":"h2t-pending-pods/#2-reason-node-and-pod-affinity-rules-applied-to-the-pod","text":"Error ==> 1 node(s) didn't match pod affinity/anti-affinity If you do not carefully configure pod affinity, node affinity and pod anti-affinity with equal-priority pods, the pod might not be scheduled and ends up in pending/waiting state Depending on your pod priority and preemption settings, the scheduler might not be able to find an appropriate node for a pod without violating affinity requirements.","title":"2. Reason: Node and pod affinity rules applied to the pod"},{"location":"h2t-pending-pods/#how-can-this-be-resolved_1","text":"Pod affinity, pod anti-affinity and node affinity allow you to constrain which nodes your pod is eligible to be scheduled on based on the key/value labels on other pods. Here is a sample configuration to help understand the application of affinity rules to nodes and pods Sample pod configuration with a node selector rule : spec: nodeSelector: beta.kubernetes.io/os: linux node-role.kubernetes.io/worker: '' type: user-node Sample pod configuration with a node affinity preferred rule : spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: e2e-az-EastWest operator: In values: - e2e-az-East - e2e-az-West Using node selectors and node affinity in the same pod configuration, note the following: If you configure both nodeSelector and nodeAffinity, both conditions must be satisfied for the pod to be scheduled onto a candidate node. If you specify multiple nodeSelectorTerms associated with nodeAffinity types, then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied. If you specify multiple matchExpressions associated with nodeSelectorTerms, then the pod can be scheduled onto a node only if all matchExpressions are satisfied.","title":"How can this be resolved?"},{"location":"h2t-pending-pods/#3-reason-applying-taints-and-tolerations","text":"Error ==> 1 node(s) had taints that the pod didn't tolerate When a taint is applied to a node, it repels the scheduling of a pod on that node unless it matches the taint with its tolerations.","title":"3. Reason: Applying taints and tolerations"},{"location":"h2t-pending-pods/#how-can-this-be-resolved_2","text":"Ensure that the tolerations are applied to the pods such that they match the taints of the nodes to end up being scheduled on the desired one. The taints applied to a node can be checked through oc describe node node-name | grep -i taint . The pod toleration in the respective yaml file can be verified to check if the pod can be scheduled on that particular node. Example : If a particular node has the following taints: oc describe node node-1 | grep -i taint Taints: node.alpha.kubernetes.io/not-ready:NoExecute The pod with the appropriate tolerations would be schedules on node-1 such as: tolerations: - key: \"node.alpha.kubernetes.io/not-ready\" operator: \"Exists\" effect: \"NoExecute\"","title":"How can this be resolved?"},{"location":"h2t-pending-pods/#4-reason-issues-with-container-engine-on-the-host","text":"Error ==> failed to pull image The specific image being used by the pod might not be available in the repository it is being pulled from. As an example Normal Pulling 10s (x4 over 94s) kubelet, worker-1.prashanth-3d14.redhat.com Pulling image \"hello-openshift\" Warning Failed 9s (x4 over 93s) kubelet, worker-1.prashanth-3d14.redhat.com Failed to pull image \"hello-openshift\": rpc error: code = Unknown desc = Error reading manifest latest in docker.io/hello-openshift/hello: errors","title":"4. Reason: Issues with Container engine on the host"},{"location":"h2t-pending-pods/#how-can-this-be-resolved_3","text":"The following can be checked: Make sure that you have the name of the image correct. Does the image exist in the repository being pulled from? Have you checked if the image is pushed to the repository? Run a manual podman/docker pull image on your machine to see if the image can be pulled.","title":"How can this be resolved?"},{"location":"h2t-pending-pods/#5-reason-persistentvolumeclaims","text":"Error ==> pod has unbound immediate PersistentVolumeClaims Pod can get stuck in pending state because the pod may have unbound immediate PersistentVolumeClaims. Can be checked with command - oc describe pod podname and oc get pvc -n namespace shows status of pvc in unbound state. As an example. Warning FailedScheduling 22s (x2 over 22s) default-scheduler pod has unbound immediate PersistentVolumeClaims","title":"5. Reason: PersistentVolumeClaims"},{"location":"h2t-pending-pods/#how-can-this-be-resolved_4","text":"The OCP cluster inspects the persistent volume claim to find the bound volume and mounts that volume for a Pod. For those persistent volumes that support multiple access modes, you must specify which mode applies when you use the claim as a volume in a Pod, in other words the 'accesModes' mentioned in the persistent volume claim of a given pod must match any existing persistent volume for the pod to be scheduled. The claims remain unbound indefinitely if a matching volume does not exist or cannot be created with any available provisioner servicing a storage class. Consider the following PV to be existing on the cluster. Persitent volume specification: apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain For a pod to be scheduled on a given node in the cluster, the PVC should consist of the following specifications. Persistent Volume Claim specifications for a pod: kind: PersistentVolumeClaim metadata: name: nfs-claim1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi","title":"How can this be resolved?"},{"location":"h2t-pending-pods/#6-reason-using-host-port","text":"Error ==> node(s) didn't have free' ports for the requested pod ports When a pod uses a 'hostPort' for any of its containers, there are a limited number of places that the pod can be scheduled. The following error would be faced. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 6s (x25 over 34s) default-scheduler 0/9 nodes are available: 4 node(s) didn't have free ports for the requested pod ports, 7 node(s) didn't match node selector.","title":"6. Reason: Using host port"},{"location":"h2t-pending-pods/#how-can-this-be-resolved_5","text":"The below command outputs the container ports in all namespaces. oc get po --all-namespaces -o=jsonpath=\"{range .items[*]}{.spec.nodeName}{'\\t'}{.spec.hostNetwork}{'\\t'}{.spec.hostNetwork}{'\\t'}{.spec.containers..containerPort}{'\\n'}{end}\" Using the above command, the occupied ports for the exisiting containers on all the nodes can be checked and if the port being requested is already occupied, the hostPort for the particular container can be modified in the requested pod configuration to use another port.","title":"How can this be resolved?"},{"location":"h2t-resource-exhaustion/","text":"Resource Exhaustion Introduction As a developer or an Ops, you occasionally get into this situation where pods or API calls fail and resume working seemingly randomly. You might notice there are a lots of \"Evicted\" and/or \"Pending\" pods, or high restart counts on pods, or your oc commands take longer to respond or even fail sometimes, and everything seems to point to the system is busy, under heavy load too much stuff on it. This document explains how to verify, diagnose, and offer what could be done about this \"resource exhaustion\" situation on an OpenShift cluster. How to verify this situation Resource exhaustion is the condition that happens when the resources required to execute an action are entirely or nearly expended, preventing that action from occurring.The most common outcome of resource exhaustion is denial of service. oc adm top nodes is one way to see how tight CPU and Memory resources are for the cluster nodes. Example output below You will get to know about resource exhaustion condition using this command $ oc adm top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\\n node-1 297m 29% 4263Mi 55% node-0 55m 5% 1201Mi 15% infra-1 85m 8% 1319Mi 17% infra-0 182m 18% 2524Mi 32% master-0 178m 8% 2584Mi 16% Other indications that one could suspect of this situation are failing pods, especially in \"Evicted\" state. Use oc get pods -A | egrep -v 'Completed|Running' and see what and how many are failing. Sometimes, \"Pending\" state could be attributed to resource exhaustion, as those pods are having hard time finding a free node to run on. Basics of scheduling pods When you specify a Pod, you can optionally specify how much of each resource a Container needs. The most common resources to specify are CPU and memory . CPU is specified in units of Kubernetes CPUs, Memory is specified in units of bytes. When you specify the resource request for Containers in a Pod, the scheduler uses this information to decide which node to place the Pod on. When you specify a resource limit for a Container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set. The kubelet also reserves at least the request amount of that system resource specifically for that container to use. For example, if you set a memory request of 256 MiB for a container, and that container is in a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use more RAM. If you set a memory limit of 4GiB for that Container, the kubelet (and container runtime) enforce the limit. The runtime prevents the container from using more than the configured resource limit. For example when a process in the container tries to consume more than the allowed amount of memory, the system kernel terminates the process that attempted the allocation, with an out of memory (OOM) error. In order to monitor this, you always have to look at the use of memory compared to the limit. Percentage of the node memory used by a pod is usually a bad indicator as it gives no indication on how close to the limit the memory usage is. In Kubernetes, limits are applied to containers, not pods, so monitor the memory usage of a container vs. the limit of that container. Pod Eviction Pods could be evicted as part of kubelet's attempt to keep the node from going down completely. See sample messages below. Example : Reason : Evicted Warning : Evicted 125m kubelet, worker-0.shivani4-9998.ocp-44.com The node had condition: [DiskPressure]. Reason : Evicted Warning : Evicted 126m kubelet, worker-0.shivani4-9998.ocp-44.com The node was low on resource: ephemeral-storage. Container certified-operators was using 44156Ki,which exceeds its request of 0. Kubernetes has a way of determining which pods to kill and evict in what order and trying to mitigate similar resource tightness in the future. It's built around the idea of garbage collection and QoS (Quality of Service) classes. See these pages for more details; https://sysdig.com/blog/kubernetes-pod-evicted/ https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/ Signs of nodes running out of memory The kubelet can support the ability to trigger eviction decisions on the signals described below. The value of each signal is described in the description column based on the kubelet summary API. Eviction Signal Description memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet nodefs.available nodefs.available nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree imagefs.available imagefs.available := node.stats.runtime.imagefs.available imagefs.inodesFree magefs.inodesFree := node.stats.runtime.imagefs.inodesFree This link explains further details about eviction signals and how to configure out of resource handling with kubelet. https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-signals When does oc describe node show MemoryPressure=true? oc describe node show MemoryPressure=true when Kubelet has insufficient memory. When a node in a Kubernetes cluster is running out of memory or disk, it activates a flag signaling that it is under pressure. This blocks any new allocation in the node and starts the eviction process.At that moment, kubelet starts to reclaim resources, killing containers and declaring pods as failed until the resource usage is under the eviction threshold again. Ephemeral storage \"Ephemeral\" means that there is no long-term guarantee about durability. Kubernetes supports two ways to configure local ephemeral storage on a node: Single file system Two file system Setting requests and limits for local ephemeral storage You can use ephemeral-storage for managing local ephemeral storage. Each Container of a Pod can specify one or more of the following: 1. spec.containers[].resources.limits.ephemeral-storage 2. spec.containers[].resources.requests.ephemeral-storage To know more in detail about ephemeral storage refer link provided below https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#:~:text=Ephemeral The kubelet supports different ways to measure Pod storage use: 1. Periodic Scanning : The kubelet performs regular, schedules checks that scan each emptyDir volume, container log directory, and writeable container layer.The scan measures how much space is used. Filesystem project quota : Project quotas are an operating-system level feature for managing storage use on filesystems. With Kubernetes, you can enable project quotas for monitoring storage use. Make sure that the filesystem backing the emptyDir volumes, on the nodeprovides project quota support. For example, XFS and ext4fs offer project quotas. Restart count Restart count represents the number of times the container inside a pod has been restarted, it is based on the number of dead containers that have not yet been removed. Note that this is calculated from dead containers. command to get restart count : kubectl describe pod nginx | grep -i \"Restart\" Details about resourse allocation As mentioned earlier, a pod can specify how much system resources it needs (\"request\") and/or doesn't want to consume more than (\"limit\"). Also, a namespace can impose defaults on all pods running in it if they don't explicitly specify their resource requests/limits. Kubernetes has guided tasks that one can follow along to understand how these settings work: CPU To specify a CPU request and limit for a container,include following fields in the container resource manifest Example: resources: limits: cpu: \"1\" requests: cpu: \"0.5\" To know more in detail about allocation of cpu resources to containers and pods refer link provided below https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/ Memory To specify a Memory request and limit for a container, include following fields in the container resource manifest Example: resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" To know more in detail about allocation of memory resource to containers and pods refer link provided below https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/ Namespace defaults If a Container is created in a namespace that has a default memory limit, and the Container does not specify its own memory limit, then the Container is assigned the default memory limit. To know more about configuring default memory requests and limits for a namespace refer link provided below https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/ At node level One can also specify how much resource Kubernetes can use and how much OS/system keeps for its stability. This helps avoid overcommitment and flapping. You can control how much of a node resource is made available for scheduling in order to allow the scheduler to fully allocate a node and to prevent evictions This page describes how this all works in OpenShift clusters in more details: https://docs.openshift.com/container-platform/4.5/nodes/nodes/nodes-nodes-resources-configuring.html Overcommitment: Scheduling is based on resources requested, while quota and hard limits refer to resource limits, which can be set higher than requested resources. The difference between request and limit determines the level of overcommit . The Cluster ResourceOverride Operator is an admission webhook that allows you to control the level of overcommit and manage container density across all the nodes in your cluster. To know in detail about cluster resource override operator refer link provided below https://docs.openshift.com/container-platform/4.5/post_installation_configuration/node-tasks.html#nodes-cluster-overcommit-resource-requests_post-install-node-tasks Flapping: If the node ready status has an altering behaviour , the node is flapping. Flapping indicates that is kubelet is not ready. To know more in detail about causes of flapping , refer the link provided below https://docs.sysdig.com/en/nodes-data.html What to do to Mitigate The kubelet needs to preserve node stability when available compute resources are low. This is especially important when dealing with incompressible compute resources, such as memory or disk space.If such resources are exhausted, nodes become unstable. The kubelet can proactively monitor for and prevent total starvation of a compute resource. In those cases, the kubelet can reclaim the starved resource by proactively failing one or more Pods. When the kubelet fails a Pod, it terminates all of its containers and transitions its PodPhase to Failed. If the evicted Pod is managed by a Deployment, the Deployment will create another Pod to be scheduled by Kubernetes. To provide more reliable scheduling and minimize node resource overcommitment, each node can reserve a portion of its resources for use by all underlying node components. CPU and memory resources reserved for node components in OpenShift Container Platform are based on two node settings: Kube-reserved System-reserved If a flag is not set, it defaults to 0. If none of the flags are set, the allocated resource is set to the node\u2019s capacity as it was before the introduction of allocatable resources. For further reading on managing node resources see https://docs.openshift.com/container-platform/4.5/nodes/nodes/nodes-nodes-resources-configuring.htm The scheduler attempts to optimize the compute resource use across all nodes in your cluster. It places Pods onto specific nodes, taking the Pods' compute resource requests and nodes' available capacity into consideration. OpenShift Container Platform administrators can control the level of overcommit and manage container density on nodes. You can configure cluster-level overcommit using the Cluster Resource Override Operator. To know in detail about cluster resource override operator see https://docs.openshift.com/container-platform/4.5/nodes/clusters/nodes-cluster-overcommit.html","title":"Resource Exhaustion"},{"location":"h2t-resource-exhaustion/#resource-exhaustion","text":"","title":"Resource Exhaustion"},{"location":"h2t-resource-exhaustion/#introduction","text":"As a developer or an Ops, you occasionally get into this situation where pods or API calls fail and resume working seemingly randomly. You might notice there are a lots of \"Evicted\" and/or \"Pending\" pods, or high restart counts on pods, or your oc commands take longer to respond or even fail sometimes, and everything seems to point to the system is busy, under heavy load too much stuff on it. This document explains how to verify, diagnose, and offer what could be done about this \"resource exhaustion\" situation on an OpenShift cluster.","title":"Introduction"},{"location":"h2t-resource-exhaustion/#how-to-verify-this-situation","text":"Resource exhaustion is the condition that happens when the resources required to execute an action are entirely or nearly expended, preventing that action from occurring.The most common outcome of resource exhaustion is denial of service. oc adm top nodes is one way to see how tight CPU and Memory resources are for the cluster nodes.","title":"How to verify this situation"},{"location":"h2t-resource-exhaustion/#example-output-below","text":"You will get to know about resource exhaustion condition using this command $ oc adm top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%\\n node-1 297m 29% 4263Mi 55% node-0 55m 5% 1201Mi 15% infra-1 85m 8% 1319Mi 17% infra-0 182m 18% 2524Mi 32% master-0 178m 8% 2584Mi 16% Other indications that one could suspect of this situation are failing pods, especially in \"Evicted\" state. Use oc get pods -A | egrep -v 'Completed|Running' and see what and how many are failing. Sometimes, \"Pending\" state could be attributed to resource exhaustion, as those pods are having hard time finding a free node to run on.","title":"Example output below"},{"location":"h2t-resource-exhaustion/#basics-of-scheduling-pods","text":"When you specify a Pod, you can optionally specify how much of each resource a Container needs. The most common resources to specify are CPU and memory . CPU is specified in units of Kubernetes CPUs, Memory is specified in units of bytes. When you specify the resource request for Containers in a Pod, the scheduler uses this information to decide which node to place the Pod on. When you specify a resource limit for a Container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set. The kubelet also reserves at least the request amount of that system resource specifically for that container to use. For example, if you set a memory request of 256 MiB for a container, and that container is in a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use more RAM. If you set a memory limit of 4GiB for that Container, the kubelet (and container runtime) enforce the limit. The runtime prevents the container from using more than the configured resource limit. For example when a process in the container tries to consume more than the allowed amount of memory, the system kernel terminates the process that attempted the allocation, with an out of memory (OOM) error. In order to monitor this, you always have to look at the use of memory compared to the limit. Percentage of the node memory used by a pod is usually a bad indicator as it gives no indication on how close to the limit the memory usage is. In Kubernetes, limits are applied to containers, not pods, so monitor the memory usage of a container vs. the limit of that container.","title":"Basics of scheduling pods"},{"location":"h2t-resource-exhaustion/#pod-eviction","text":"Pods could be evicted as part of kubelet's attempt to keep the node from going down completely. See sample messages below. Example : Reason : Evicted Warning : Evicted 125m kubelet, worker-0.shivani4-9998.ocp-44.com The node had condition: [DiskPressure]. Reason : Evicted Warning : Evicted 126m kubelet, worker-0.shivani4-9998.ocp-44.com The node was low on resource: ephemeral-storage. Container certified-operators was using 44156Ki,which exceeds its request of 0. Kubernetes has a way of determining which pods to kill and evict in what order and trying to mitigate similar resource tightness in the future. It's built around the idea of garbage collection and QoS (Quality of Service) classes. See these pages for more details; https://sysdig.com/blog/kubernetes-pod-evicted/ https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/","title":"Pod Eviction"},{"location":"h2t-resource-exhaustion/#signs-of-nodes-running-out-of-memory","text":"The kubelet can support the ability to trigger eviction decisions on the signals described below. The value of each signal is described in the description column based on the kubelet summary API. Eviction Signal Description memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet nodefs.available nodefs.available nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree imagefs.available imagefs.available := node.stats.runtime.imagefs.available imagefs.inodesFree magefs.inodesFree := node.stats.runtime.imagefs.inodesFree This link explains further details about eviction signals and how to configure out of resource handling with kubelet. https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-signals","title":"Signs of nodes running out of memory"},{"location":"h2t-resource-exhaustion/#when-does-oc-describe-node-show-memorypressuretrue","text":"oc describe node show MemoryPressure=true when Kubelet has insufficient memory. When a node in a Kubernetes cluster is running out of memory or disk, it activates a flag signaling that it is under pressure. This blocks any new allocation in the node and starts the eviction process.At that moment, kubelet starts to reclaim resources, killing containers and declaring pods as failed until the resource usage is under the eviction threshold again.","title":"When does oc describe node show MemoryPressure=true?"},{"location":"h2t-resource-exhaustion/#ephemeral-storage","text":"\"Ephemeral\" means that there is no long-term guarantee about durability. Kubernetes supports two ways to configure local ephemeral storage on a node: Single file system Two file system","title":"Ephemeral storage"},{"location":"h2t-resource-exhaustion/#setting-requests-and-limits-for-local-ephemeral-storage","text":"You can use ephemeral-storage for managing local ephemeral storage. Each Container of a Pod can specify one or more of the following: 1. spec.containers[].resources.limits.ephemeral-storage 2. spec.containers[].resources.requests.ephemeral-storage To know more in detail about ephemeral storage refer link provided below https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#:~:text=Ephemeral The kubelet supports different ways to measure Pod storage use: 1. Periodic Scanning : The kubelet performs regular, schedules checks that scan each emptyDir volume, container log directory, and writeable container layer.The scan measures how much space is used. Filesystem project quota : Project quotas are an operating-system level feature for managing storage use on filesystems. With Kubernetes, you can enable project quotas for monitoring storage use. Make sure that the filesystem backing the emptyDir volumes, on the nodeprovides project quota support. For example, XFS and ext4fs offer project quotas.","title":"Setting requests and limits for local ephemeral storage"},{"location":"h2t-resource-exhaustion/#restart-count","text":"Restart count represents the number of times the container inside a pod has been restarted, it is based on the number of dead containers that have not yet been removed. Note that this is calculated from dead containers. command to get restart count : kubectl describe pod nginx | grep -i \"Restart\"","title":"Restart count"},{"location":"h2t-resource-exhaustion/#details-about-resourse-allocation","text":"As mentioned earlier, a pod can specify how much system resources it needs (\"request\") and/or doesn't want to consume more than (\"limit\"). Also, a namespace can impose defaults on all pods running in it if they don't explicitly specify their resource requests/limits. Kubernetes has guided tasks that one can follow along to understand how these settings work:","title":"Details about resourse allocation"},{"location":"h2t-resource-exhaustion/#cpu","text":"To specify a CPU request and limit for a container,include following fields in the container resource manifest Example: resources: limits: cpu: \"1\" requests: cpu: \"0.5\" To know more in detail about allocation of cpu resources to containers and pods refer link provided below https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/","title":"CPU"},{"location":"h2t-resource-exhaustion/#memory","text":"To specify a Memory request and limit for a container, include following fields in the container resource manifest Example: resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" To know more in detail about allocation of memory resource to containers and pods refer link provided below https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/","title":"Memory"},{"location":"h2t-resource-exhaustion/#namespace-defaults","text":"If a Container is created in a namespace that has a default memory limit, and the Container does not specify its own memory limit, then the Container is assigned the default memory limit. To know more about configuring default memory requests and limits for a namespace refer link provided below https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/","title":"Namespace defaults"},{"location":"h2t-resource-exhaustion/#at-node-level","text":"One can also specify how much resource Kubernetes can use and how much OS/system keeps for its stability. This helps avoid overcommitment and flapping. You can control how much of a node resource is made available for scheduling in order to allow the scheduler to fully allocate a node and to prevent evictions This page describes how this all works in OpenShift clusters in more details: https://docs.openshift.com/container-platform/4.5/nodes/nodes/nodes-nodes-resources-configuring.html Overcommitment: Scheduling is based on resources requested, while quota and hard limits refer to resource limits, which can be set higher than requested resources. The difference between request and limit determines the level of overcommit . The Cluster ResourceOverride Operator is an admission webhook that allows you to control the level of overcommit and manage container density across all the nodes in your cluster. To know in detail about cluster resource override operator refer link provided below https://docs.openshift.com/container-platform/4.5/post_installation_configuration/node-tasks.html#nodes-cluster-overcommit-resource-requests_post-install-node-tasks Flapping: If the node ready status has an altering behaviour , the node is flapping. Flapping indicates that is kubelet is not ready. To know more in detail about causes of flapping , refer the link provided below https://docs.sysdig.com/en/nodes-data.html","title":"At node level"},{"location":"h2t-resource-exhaustion/#what-to-do-to-mitigate","text":"The kubelet needs to preserve node stability when available compute resources are low. This is especially important when dealing with incompressible compute resources, such as memory or disk space.If such resources are exhausted, nodes become unstable. The kubelet can proactively monitor for and prevent total starvation of a compute resource. In those cases, the kubelet can reclaim the starved resource by proactively failing one or more Pods. When the kubelet fails a Pod, it terminates all of its containers and transitions its PodPhase to Failed. If the evicted Pod is managed by a Deployment, the Deployment will create another Pod to be scheduled by Kubernetes. To provide more reliable scheduling and minimize node resource overcommitment, each node can reserve a portion of its resources for use by all underlying node components. CPU and memory resources reserved for node components in OpenShift Container Platform are based on two node settings: Kube-reserved System-reserved If a flag is not set, it defaults to 0. If none of the flags are set, the allocated resource is set to the node\u2019s capacity as it was before the introduction of allocatable resources. For further reading on managing node resources see https://docs.openshift.com/container-platform/4.5/nodes/nodes/nodes-nodes-resources-configuring.htm The scheduler attempts to optimize the compute resource use across all nodes in your cluster. It places Pods onto specific nodes, taking the Pods' compute resource requests and nodes' available capacity into consideration. OpenShift Container Platform administrators can control the level of overcommit and manage container density on nodes. You can configure cluster-level overcommit using the Cluster Resource Override Operator. To know in detail about cluster resource override operator see https://docs.openshift.com/container-platform/4.5/nodes/clusters/nodes-cluster-overcommit.html","title":"What to do to Mitigate"},{"location":"h2t-scc/","text":"Troubleshoot Security Context Constraints (SCC) This document contains information regarding troubleshooting SCC related issues. Document includes the initial diagnosis step which helps find the actual error. To solve the SCC related issues, you need a basic understanding of the SCC, and we will explain what SCC is along with its role in the cluster. Initial Diagnosis $ oc describe pod <pod-name> | grep scc This command show type of SCC used in the pod deployment. Knowing SCC name will let us know about control permissions for a pod. These permissions include actions that a pod, a collection of containers, can perform, and what resources it can access. $ oc describe scc <scc-name> Examine the pod SCC using the above command. The command output consists of permissions include actions that a pod, a collection of containers, can perform, and what resources it can access. Sample Issue The pod was unable to mount the host volume due to user doesn't have right permissions. The error is pasted below. Error: | CAUSE: current user doesn't have permissions for writing to /var/lib/mongodb/data directory | DETAILS: current user id = 184, user groups: 997 0 | DETAILS: directory permissions: drwxrwsr-x owned by 0:1000360000, SELinux: system_u:object_r:svirt_sandbox_file_t:s0:c9,c19 Solution: Allowing access to SCCs with a RunAsAny FSGroup strategy can prevent users from accessing their block devices. Pods need to specify a fsGroup in order to take over their block devices. Normally, this is done when the SCC FSGroup strategy is set to MustRunAs. If a user\u2019s pod is assigned an SCC with a RunAsAny FSGroup strategy, then the user may face permission denied errors until they discover that they need to specify a fsGroup themselves. To solve the error first need to find out the scc type used by the pod then edit the SCC according to your pod requirement or change the SCC type. Editing the current SCC and add FSGroup to MustRunAs $ oc edit scc <scc-name> Edit the SCC using this command. You can edit SCC to define a set of conditions that a pod must run with in order to be accepted into the system. FSGroup Strategy: RunAsAny What is SCC? SCC is basically used for pod restriction, which means it defines the limitations for a pod, as in what actions it can perform and what all things it can access in the cluster. Advantages of using SCC. Host directories can be used as volumes. Able can set conditions for container user ID and Selinux Context of the container. Able to run containers as privileged. Able to control the use of host namespaces and networking. Able to control usage of volume types. Able to control capabilities that a container can request. FSGroup can be allocated to pod volumes. OpenShift provides a set of predefined SCC that can be used, modified, and extended by the administrator. $ oc get scc NAME PRIV CAPS HOSTDIR SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY anyuid false [] false MustRunAs RunAsAny RunAsAny RunAsAny hostaccess false [] true MustRunAs MustRunAsRange RunAsAny RunAsAny hostmount-anyuid false [] true MustRunAs RunAsAny RunAsAny RunAsAny nonroot false [] false MustRunAs MustRunAsNonRoot RunAsAny RunAsAny privileged true [] true RunAsAny RunAsAny RunAsAny RunAsAny restricted false [] false MustRunAs MustRunAsRange RunAsAny RunAsAny If one wishes to use any pre-defined scc, that can be done by simply adding the user or the group to the scc group. $ oadm policy add-user-to-scc <scc_name> <user_name> $ oadm policy add-group-to-scc <scc_name> <group_name> OpenShift guarantees that the capabilities required by a container are granted to the user that executes the container at admission time. We can not allow any container to get access to unnecessary capabilities or to run in an insecure way (e.g. privileged or as root) Adding a regular user or to a group given access to the SCC allows them to run privileged pods. There are mainly four section in SCC to control access to volumes in OpenShift. Supplemental Groups fsGroup runAsUser seLinuxOptions 1. Supplemental Groups Supplemental groups are regular Linux groups. When a process runs in the system, it runs with a user ID and group ID. These groups are used for controlling access to shared storage. Check the NFS mount using the following command. # showmount -e <nfs-server-ip-or-hostname> Export list for f21-nfs.vm: /opt/nfs * Check NFS details on the mount server using the following command. # cat /etc/exports /opt/nfs *(rw,sync,no_root_squash) Check owner of exported directory $ ls -lZ /opt/nfs -d drwxrws---. nfsnobody 2325 unconfined_u:object_r:usr_t:s0 /opt/nfs $ id nfsnobody uid = 65534(nfsnobody) gid = 454265(nfsnobody) groups = 454265(nfsnobody) The /opt/nfs/ export is accessible by UID 454265 and the group 2325. apiVersion: v1 kind: Pod ... spec: containers: - name: ... volumeMounts: - name: nfs mountPath: /usr/share/... securityContext: supplementalGroups: [2325] volumes: - name: nfs nfs: server: <nfs_server_ip_or_host> path: /opt/nfs 2. fsGroup fsGroup stands for the file system group which is used for adding container supplemental groups. Supplement group ID is used for shared storage and fsGroup is used for block storage. kind: Pod spec: containers: - name: ... securityContext: fsGroup: 2325 3. runAsUser runAsUser uses the user ID for communication. This is used in defining the container image in pod definition. A single ID user can be used in all containers, if required. While running the container, the defined ID is matched with the owner ID on the export. If the specified ID is defined outside, then it becomes global to all the containers in the pod. If it is defined with a specific pod, then it becomes specific to a single container. spec: containers: - name: ... securityContext: runAsUser: 454265 4. seLinuxOptions SELinux default when not defined in the pod definition or in the SCC. Level can be defined globally for the entire pod, or individually for each container. SElinux values, policies and values given below values: 1.Enforcing 2.permissive 3.disabled policies:1.targeted 2.minimum 3.mls(multilevel) components: Selinux user Selinux role Type Sensitivity / category spec: container: seLinuxContext: type: MustRunAs SELinuxOptions: user: <selinux-user-name> role: ... type: ... level: ...","title":"Troubleshoot Security Context Constraints (SCC)"},{"location":"h2t-scc/#troubleshoot-security-context-constraints-scc","text":"This document contains information regarding troubleshooting SCC related issues. Document includes the initial diagnosis step which helps find the actual error. To solve the SCC related issues, you need a basic understanding of the SCC, and we will explain what SCC is along with its role in the cluster.","title":"Troubleshoot Security Context Constraints (SCC)"},{"location":"h2t-scc/#initial-diagnosis","text":"$ oc describe pod <pod-name> | grep scc This command show type of SCC used in the pod deployment. Knowing SCC name will let us know about control permissions for a pod. These permissions include actions that a pod, a collection of containers, can perform, and what resources it can access. $ oc describe scc <scc-name> Examine the pod SCC using the above command. The command output consists of permissions include actions that a pod, a collection of containers, can perform, and what resources it can access.","title":"Initial Diagnosis"},{"location":"h2t-scc/#sample-issue","text":"The pod was unable to mount the host volume due to user doesn't have right permissions. The error is pasted below. Error: | CAUSE: current user doesn't have permissions for writing to /var/lib/mongodb/data directory | DETAILS: current user id = 184, user groups: 997 0 | DETAILS: directory permissions: drwxrwsr-x owned by 0:1000360000, SELinux: system_u:object_r:svirt_sandbox_file_t:s0:c9,c19 Solution: Allowing access to SCCs with a RunAsAny FSGroup strategy can prevent users from accessing their block devices. Pods need to specify a fsGroup in order to take over their block devices. Normally, this is done when the SCC FSGroup strategy is set to MustRunAs. If a user\u2019s pod is assigned an SCC with a RunAsAny FSGroup strategy, then the user may face permission denied errors until they discover that they need to specify a fsGroup themselves. To solve the error first need to find out the scc type used by the pod then edit the SCC according to your pod requirement or change the SCC type. Editing the current SCC and add FSGroup to MustRunAs $ oc edit scc <scc-name> Edit the SCC using this command. You can edit SCC to define a set of conditions that a pod must run with in order to be accepted into the system. FSGroup Strategy: RunAsAny","title":"Sample Issue"},{"location":"h2t-scc/#what-is-scc","text":"SCC is basically used for pod restriction, which means it defines the limitations for a pod, as in what actions it can perform and what all things it can access in the cluster. Advantages of using SCC. Host directories can be used as volumes. Able can set conditions for container user ID and Selinux Context of the container. Able to run containers as privileged. Able to control the use of host namespaces and networking. Able to control usage of volume types. Able to control capabilities that a container can request. FSGroup can be allocated to pod volumes. OpenShift provides a set of predefined SCC that can be used, modified, and extended by the administrator. $ oc get scc NAME PRIV CAPS HOSTDIR SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY anyuid false [] false MustRunAs RunAsAny RunAsAny RunAsAny hostaccess false [] true MustRunAs MustRunAsRange RunAsAny RunAsAny hostmount-anyuid false [] true MustRunAs RunAsAny RunAsAny RunAsAny nonroot false [] false MustRunAs MustRunAsNonRoot RunAsAny RunAsAny privileged true [] true RunAsAny RunAsAny RunAsAny RunAsAny restricted false [] false MustRunAs MustRunAsRange RunAsAny RunAsAny If one wishes to use any pre-defined scc, that can be done by simply adding the user or the group to the scc group. $ oadm policy add-user-to-scc <scc_name> <user_name> $ oadm policy add-group-to-scc <scc_name> <group_name> OpenShift guarantees that the capabilities required by a container are granted to the user that executes the container at admission time. We can not allow any container to get access to unnecessary capabilities or to run in an insecure way (e.g. privileged or as root) Adding a regular user or to a group given access to the SCC allows them to run privileged pods. There are mainly four section in SCC to control access to volumes in OpenShift. Supplemental Groups fsGroup runAsUser seLinuxOptions","title":"What is SCC?"},{"location":"h2t-scc/#1-supplemental-groups","text":"Supplemental groups are regular Linux groups. When a process runs in the system, it runs with a user ID and group ID. These groups are used for controlling access to shared storage. Check the NFS mount using the following command. # showmount -e <nfs-server-ip-or-hostname> Export list for f21-nfs.vm: /opt/nfs * Check NFS details on the mount server using the following command. # cat /etc/exports /opt/nfs *(rw,sync,no_root_squash) Check owner of exported directory $ ls -lZ /opt/nfs -d drwxrws---. nfsnobody 2325 unconfined_u:object_r:usr_t:s0 /opt/nfs $ id nfsnobody uid = 65534(nfsnobody) gid = 454265(nfsnobody) groups = 454265(nfsnobody) The /opt/nfs/ export is accessible by UID 454265 and the group 2325. apiVersion: v1 kind: Pod ... spec: containers: - name: ... volumeMounts: - name: nfs mountPath: /usr/share/... securityContext: supplementalGroups: [2325] volumes: - name: nfs nfs: server: <nfs_server_ip_or_host> path: /opt/nfs","title":"1. Supplemental Groups"},{"location":"h2t-scc/#2-fsgroup","text":"fsGroup stands for the file system group which is used for adding container supplemental groups. Supplement group ID is used for shared storage and fsGroup is used for block storage. kind: Pod spec: containers: - name: ... securityContext: fsGroup: 2325","title":"2. fsGroup"},{"location":"h2t-scc/#3-runasuser","text":"runAsUser uses the user ID for communication. This is used in defining the container image in pod definition. A single ID user can be used in all containers, if required. While running the container, the defined ID is matched with the owner ID on the export. If the specified ID is defined outside, then it becomes global to all the containers in the pod. If it is defined with a specific pod, then it becomes specific to a single container. spec: containers: - name: ... securityContext: runAsUser: 454265","title":"3. runAsUser"},{"location":"h2t-scc/#4-selinuxoptions","text":"SELinux default when not defined in the pod definition or in the SCC. Level can be defined globally for the entire pod, or individually for each container. SElinux values, policies and values given below values: 1.Enforcing 2.permissive 3.disabled policies:1.targeted 2.minimum 3.mls(multilevel) components: Selinux user Selinux role Type Sensitivity / category spec: container: seLinuxContext: type: MustRunAs SELinuxOptions: user: <selinux-user-name> role: ... type: ... level: ...","title":"4. seLinuxOptions"},{"location":"kruize/","text":"Deployment of Kruize on Openshift on Power Introduction There were times when we used to have challenges in right sizing and optimizing containers which leads to these drawback - When applications were not sized appropriately in a k8s cluster, they either waste resources (oversized) or worse get terminated (undersized), either way could result in loss of revenue. Quick fix for this issue is Kruize . Kruize is an opensource tool which monitors application containers for resource usage. It has an analysis engine which predicts the right size for the containers that are being monitored and offers recommendations on the right CPU and Memory request and limit values. This helps IT admins to review and apply the recommendations knowing that their applications and even the cluster is optimally sized. The install steps that kruize has generally worked on an OpenShift cluster on IBM Power Systems, but you might find below some tweaks we had to make. NOTE: It is recommended to run the pods that are being monitored by kruize with no requests and limits set (or with very high limits set). This helps kruize to understand what is the real usage and hence can recommend the right set of values. Steps for deploying kruize on IBM Power Systems 1. Clone the kruize github repository : $ git clone https://github.com/kruize/kruize.git $ cd kruize Now make the following changes in manifests/kruize.yaml_template - apiVersion: extensions/v1beta1 + apiVersion: apps/v1 kind: Deployment metadata: name: kruize labels: app: kruize spec: replicas: 1 + selector: + matchLabels: + app: kruize + name: kruize + app.kubernetes.io/name: \"kruize\" template: metadata: labels: 2. Run the deploy.sh script to start deployment : $ ./deploy.sh -c openshift Sample output: ### Installing kruize for OpenShift WARNING: This will create a Kruize ServiceMonitor object in the openshift-monitoring namespace WARNING: This is currently not recommended for production Create ServiceMonitor object and continue installation?(y/n)? yes Info: Checking pre requisites for OpenShift...done Info: Logging in to OpenShift cluster... Authentication required for https://api.kruize.cp.fyre.ibm.com:6443 (openshift) Username: kubeadmin Password: Login successful. You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". Info: Setting Prometheus URL as https://prometheus-k8s-openshift-monitoring.apps.kruize.cp.fyre.ibm.com Info: Deploying kruize yaml to OpenShift cluster Now using project \"openshift-monitoring\" on server \"https://api.kruize.cp.fyre.ibm.com:6443\". deployment.apps/kruize created service/kruize configured Info: Waiting for kruize to come up... kruize-6cfc8f4bbf-956vq 1/1 ContainerCreating 0 11m kruize-f54c97d6f-78l7m 1/1 Running 0 4s Info: kruize deploy succeeded: Running kruize-6cfc8f4bbf-956vq 1/1 ContainerCreating 0 11m kruize-f54c97d6f-78l7m 1/1 Running 0 4s NOTE: If kruize pod is not in Running state then check the logs of the failed kruize pod by executing oc logs -f <kruize_pod_name> If log says execution format error then the image would be of some other architecture(probably amd64) . If that's the case, you can build your own power(ppc64le) image by using build.sh script present here But before running ./build.sh make sure you have docker installed on your system . 3. Once the deployment is complete, create a route in order to access the Kruize API $ cat route.yaml kind: Route apiVersion: route.openshift.io/v1 metadata: name: kruize namespace: openshift-monitoring labels: app: kruize annotations: openshift.io/host.generated: 'true' spec: host: kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com to: kind: Service name: kruize weight: 100 port: targetPort: http wildcardPolicy: None status: ingress: - host: kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com routerName: default wildcardPolicy: None routerCanonicalHostname: apps.kruize.cp.fyre.ibm.com NOTE: Change the values of host and routerCanonicalHostname according to your cluster. Run: oc create -f route.yaml 4. Use the API to view recommendations List recommendations for all applications by running the following command : $ curl http://< host_of_your_cluster >/recommendations?listApplications Example : [root@kruize-inf ~]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?listApplications [ { \"application_name\": \"kube-state-metrics\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"node-exporter\", \"resources\": { \"requests\": { \"memory\": \"125.5M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"150.7M\", \"cpu\": 1.0 } } }, { \"application_name\": \"kruize\", \"resources\": { \"requests\": { \"memory\": \"158.2M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"278.1M\", \"cpu\": 1.0 } } }, { \"application_name\": \"thanos-querier\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"prometheus-operator\", \"resources\": { \"requests\": { \"memory\": \"133.8M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"160.6M\", \"cpu\": 1.0 } } } ] NOTE: This is initial output before adding any load on application, and it shows all applications that are deployed in openshift-monitoring namespace because Kruize is also deployed in this namespace. List recommendations for single application: $ curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=kruize [ { \"application_name\": \"kruize\", \"resources\": { \"requests\": { \"memory\": \"158.2M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"278.1M\", \"cpu\": 1.0 } } } ] 5. Label the pod to monitor your application: Kruize by default monitors openshift-monitoring namespace. In order to monitor your application, run the following command: $ oc label pod -n <NS> <POD_NAME> app.kubernetes.io/name=<ANY_NAME> NS stands for namespace name Examples 1. To monitor deployment for an acmeair application: [root@kruize-inf ~]# git clone https://github.ibm.com/powercloud/acmeair.git Cloning into 'acmeair'... remote: Enumerating objects: 31, done. remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 31 Unpacking objects: 100% (31/31), done. [root@kruize-inf ~]# cd acmeair/ [root@kruize-inf ~]# git checkout powervs Branch 'powervs' set up to track remote branch 'powervs' from 'origin'. Switched to a new branch 'powervs' [root@kruize-inf acmeair]# ls acmeair.PNG deploy.sh openshift-manifest README.md [root@kruize-inf acmeair]# ./deploy.sh Route Host=acmeair.apps.kruize.cp.fyre.ibm.com Now using project \"acme\" on server \"https://api.kruize.cp.fyre.ibm.com:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname Already on project \"acme\" on server \"https://api.kruize.cp.fyre.ibm.com:6443\". Adding quay.io/pravin_dsilva/ for main service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for main service Adding quay.io/pravin_dsilva/ for auth service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for auth service Adding quay.io/pravin_dsilva/ for booking service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for booking service Adding quay.io/pravin_dsilva/ for customer service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for customer service Adding quay.io/pravin_dsilva/ for flight service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for flight service route.route.openshift.io/acmeair-auth-route created deployment.apps/acmeair-authservice created service/acmeair-auth-service created route.route.openshift.io/acmeair-booking-route created deployment.apps/acmeair-bookingservice created service/acmeair-booking-service created service/acmeair-booking-db created deployment.apps/acmeair-booking-db created route.route.openshift.io/acmeair-customer-route created deployment.apps/acmeair-customerservice created service/acmeair-customer-service created service/acmeair-customer-db created deployment.apps/acmeair-customer-db created route.route.openshift.io/acmeair-flight-route created deployment.apps/acmeair-flightservice created service/acmeair-flight-service created service/acmeair-flight-db created deployment.apps/acmeair-flight-db created route.route.openshift.io/acmeair-main-route created deployment.apps/acmeair-mainservice created service/acmeair-main-service created Acme Air Deployment complete. Access the application at acmeair.apps.kruize.cp.fyre.ibm.com/acmeair [root@kruize-inf acmeair]# oc get pods NAME READY STATUS RESTARTS AGE acmeair-authservice-55648d8445-4k4r4 1/1 Running 0 57m acmeair-booking-db-6ffc689d55-kxs9r 1/1 Running 0 57m acmeair-bookingservice-76fc8f7f84-5vsml 1/1 Running 0 57m acmeair-customer-db-ff6bcc8fc-qzxxp 1/1 Running 0 57m acmeair-customerservice-77b69b8f58-wvwrb 1/1 Running 0 57m acmeair-flight-db-6f9ff4dd6f-zczrl 1/1 Running 0 57m acmeair-flightservice-7d47988b8b-hwqjg 1/1 Running 0 57m acmeair-mainservice-7f7ff489d-qhss7 1/1 Running 0 57m In order to monitor your application, label the pod as mentioned in Step5: [root@kruize-inf acmeair]# oc label pod -n acme acmeair-customer-db-ff6bcc8fc-qzxxp app.kubernetes.io/name=acmeair-customer-db pod/acmeair-customer-db-ff6bcc8fc-qzxxp labeled List recommendations: [root@kruize-inf acmeair]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=acmeair-customer-db [ { \"application_name\": \"acmeair-customer-db\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } } ] NOTE: The above outputs are before adding any load on the application. After adding load: curl http://acmeair.apps.kruize.cp.fyre.ibm.com:6443/booking/loader/load curl http://acmeair.apps.kruize.cp.fyre.ibm.com:6443/flight/loader/load curl http://acmeair.apps.kruize.cp.fyre.ibm.com:6443/customer/loader/load?numCustomers=100000 To list recommendations for single application: [root@kruize-inf acmeair]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=acmeair-customer-db [ { \"application_name\": \"acmeair-customer-db\", \"resources\": { \"requests\": { \"memory\": \"194.6M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"233.5M\", \"cpu\": 1.0 } } } ] List of recommendations on all the applications: [root@kruize-inf acmeair]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?listApplications [ { \"application_name\": \"kube-state-metrics\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"node-exporter\", \"resources\": { \"requests\": { \"memory\": \"125.8M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"151.4M\", \"cpu\": 1.0 } } }, { \"application_name\": \"kruize\", \"resources\": { \"requests\": { \"memory\": \"175.9M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"278.1M\", \"cpu\": 1.0 } } }, { \"application_name\": \"acmeair-customer-db\", \"resources\": { \"requests\": { \"memory\": \"194.6M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"233.5M\", \"cpu\": 1.0 } } }, { \"application_name\": \"thanos-querier\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"prometheus-operator\", \"resources\": { \"requests\": { \"memory\": \"134.1M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"161.6M\", \"cpu\": 1.0 } } } ] 2. To monitor osd pod in Openshift Container Storage: [root@kruize-inf ~]# oc get pods -n openshift-storage |grep osd NAME READY STATUS RESTARTS AGE rook-ceph-osd-0-6499496d58-pwml8 1/1 Running 0 12m rook-ceph-osd-1-5bd4d44b6f-dd6nq 1/1 Running 22 2d15h rook-ceph-osd-2-655f579dc7-65c7p 1/1 Running 0 141m Label the osd pod which you want to monitor: [root@kruize-inf ~]# oc label pod rook-ceph-osd-1-5bd4d44b6f-dd6nq -n openshift-storage app.kubernetes.io/name=rook-ceph-osd-1 pod/rook-ceph-osd-1-5bd4d44b6f-dd6nq labeled View Recommendations: Before adding load: [root@kruize-inf ~]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=rook-ceph-osd-1 [ { \"application_name\": \"rook-ceph-osd-1\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } } ] After running some tests/adding load: [root@kruize-inf ~]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=rook-ceph-osd-1 [ { \"application_name\": \"rook-ceph-osd-1\", \"resources\": { \"requests\": { \"memory\": \"3427.2M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"6415.4M\", \"cpu\": 1.0 } } } ]","title":"Deployment of Kruize on Openshift on Power"},{"location":"kruize/#deployment-of-kruize-on-openshift-on-power","text":"","title":"Deployment of Kruize on Openshift on Power"},{"location":"kruize/#introduction","text":"There were times when we used to have challenges in right sizing and optimizing containers which leads to these drawback - When applications were not sized appropriately in a k8s cluster, they either waste resources (oversized) or worse get terminated (undersized), either way could result in loss of revenue. Quick fix for this issue is Kruize . Kruize is an opensource tool which monitors application containers for resource usage. It has an analysis engine which predicts the right size for the containers that are being monitored and offers recommendations on the right CPU and Memory request and limit values. This helps IT admins to review and apply the recommendations knowing that their applications and even the cluster is optimally sized. The install steps that kruize has generally worked on an OpenShift cluster on IBM Power Systems, but you might find below some tweaks we had to make. NOTE: It is recommended to run the pods that are being monitored by kruize with no requests and limits set (or with very high limits set). This helps kruize to understand what is the real usage and hence can recommend the right set of values.","title":"Introduction"},{"location":"kruize/#steps-for-deploying-kruize-on-ibm-power-systems","text":"","title":"Steps for deploying kruize on IBM Power Systems "},{"location":"kruize/#1-clone-the-kruize-github-repository","text":"$ git clone https://github.com/kruize/kruize.git $ cd kruize Now make the following changes in manifests/kruize.yaml_template - apiVersion: extensions/v1beta1 + apiVersion: apps/v1 kind: Deployment metadata: name: kruize labels: app: kruize spec: replicas: 1 + selector: + matchLabels: + app: kruize + name: kruize + app.kubernetes.io/name: \"kruize\" template: metadata: labels:","title":"1. Clone the kruize github repository :"},{"location":"kruize/#2-run-the-deploysh-script-to-start-deployment","text":"$ ./deploy.sh -c openshift Sample output: ### Installing kruize for OpenShift WARNING: This will create a Kruize ServiceMonitor object in the openshift-monitoring namespace WARNING: This is currently not recommended for production Create ServiceMonitor object and continue installation?(y/n)? yes Info: Checking pre requisites for OpenShift...done Info: Logging in to OpenShift cluster... Authentication required for https://api.kruize.cp.fyre.ibm.com:6443 (openshift) Username: kubeadmin Password: Login successful. You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". Info: Setting Prometheus URL as https://prometheus-k8s-openshift-monitoring.apps.kruize.cp.fyre.ibm.com Info: Deploying kruize yaml to OpenShift cluster Now using project \"openshift-monitoring\" on server \"https://api.kruize.cp.fyre.ibm.com:6443\". deployment.apps/kruize created service/kruize configured Info: Waiting for kruize to come up... kruize-6cfc8f4bbf-956vq 1/1 ContainerCreating 0 11m kruize-f54c97d6f-78l7m 1/1 Running 0 4s Info: kruize deploy succeeded: Running kruize-6cfc8f4bbf-956vq 1/1 ContainerCreating 0 11m kruize-f54c97d6f-78l7m 1/1 Running 0 4s NOTE: If kruize pod is not in Running state then check the logs of the failed kruize pod by executing oc logs -f <kruize_pod_name> If log says execution format error then the image would be of some other architecture(probably amd64) . If that's the case, you can build your own power(ppc64le) image by using build.sh script present here But before running ./build.sh make sure you have docker installed on your system .","title":"2. Run the deploy.sh script to start deployment\u00a0:"},{"location":"kruize/#3-once-the-deployment-is-complete-create-a-route-in-order-to-access-the-kruize-api","text":"$ cat route.yaml kind: Route apiVersion: route.openshift.io/v1 metadata: name: kruize namespace: openshift-monitoring labels: app: kruize annotations: openshift.io/host.generated: 'true' spec: host: kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com to: kind: Service name: kruize weight: 100 port: targetPort: http wildcardPolicy: None status: ingress: - host: kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com routerName: default wildcardPolicy: None routerCanonicalHostname: apps.kruize.cp.fyre.ibm.com NOTE: Change the values of host and routerCanonicalHostname according to your cluster. Run: oc create -f route.yaml","title":"3. Once the deployment is complete, create a route in order to access the Kruize API"},{"location":"kruize/#4-use-the-api-to-view-recommendations","text":"List recommendations for all applications by running the following command : $ curl http://< host_of_your_cluster >/recommendations?listApplications Example : [root@kruize-inf ~]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?listApplications [ { \"application_name\": \"kube-state-metrics\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"node-exporter\", \"resources\": { \"requests\": { \"memory\": \"125.5M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"150.7M\", \"cpu\": 1.0 } } }, { \"application_name\": \"kruize\", \"resources\": { \"requests\": { \"memory\": \"158.2M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"278.1M\", \"cpu\": 1.0 } } }, { \"application_name\": \"thanos-querier\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"prometheus-operator\", \"resources\": { \"requests\": { \"memory\": \"133.8M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"160.6M\", \"cpu\": 1.0 } } } ] NOTE: This is initial output before adding any load on application, and it shows all applications that are deployed in openshift-monitoring namespace because Kruize is also deployed in this namespace. List recommendations for single application: $ curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=kruize [ { \"application_name\": \"kruize\", \"resources\": { \"requests\": { \"memory\": \"158.2M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"278.1M\", \"cpu\": 1.0 } } } ]","title":"4. Use the API to view recommendations"},{"location":"kruize/#5-label-the-pod-to-monitor-your-application","text":"Kruize by default monitors openshift-monitoring namespace. In order to monitor your application, run the following command: $ oc label pod -n <NS> <POD_NAME> app.kubernetes.io/name=<ANY_NAME> NS stands for namespace name","title":"5. Label the pod to monitor your application:"},{"location":"kruize/#examples","text":"","title":"Examples"},{"location":"kruize/#1-to-monitor-deployment-for-an-acmeair-application","text":"[root@kruize-inf ~]# git clone https://github.ibm.com/powercloud/acmeair.git Cloning into 'acmeair'... remote: Enumerating objects: 31, done. remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 31 Unpacking objects: 100% (31/31), done. [root@kruize-inf ~]# cd acmeair/ [root@kruize-inf ~]# git checkout powervs Branch 'powervs' set up to track remote branch 'powervs' from 'origin'. Switched to a new branch 'powervs' [root@kruize-inf acmeair]# ls acmeair.PNG deploy.sh openshift-manifest README.md [root@kruize-inf acmeair]# ./deploy.sh Route Host=acmeair.apps.kruize.cp.fyre.ibm.com Now using project \"acme\" on server \"https://api.kruize.cp.fyre.ibm.com:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname Already on project \"acme\" on server \"https://api.kruize.cp.fyre.ibm.com:6443\". Adding quay.io/pravin_dsilva/ for main service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for main service Adding quay.io/pravin_dsilva/ for auth service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for auth service Adding quay.io/pravin_dsilva/ for booking service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for booking service Adding quay.io/pravin_dsilva/ for customer service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for customer service Adding quay.io/pravin_dsilva/ for flight service Patching Route Host: acmeair.apps.kruize.cp.fyre.ibm.com for flight service route.route.openshift.io/acmeair-auth-route created deployment.apps/acmeair-authservice created service/acmeair-auth-service created route.route.openshift.io/acmeair-booking-route created deployment.apps/acmeair-bookingservice created service/acmeair-booking-service created service/acmeair-booking-db created deployment.apps/acmeair-booking-db created route.route.openshift.io/acmeair-customer-route created deployment.apps/acmeair-customerservice created service/acmeair-customer-service created service/acmeair-customer-db created deployment.apps/acmeair-customer-db created route.route.openshift.io/acmeair-flight-route created deployment.apps/acmeair-flightservice created service/acmeair-flight-service created service/acmeair-flight-db created deployment.apps/acmeair-flight-db created route.route.openshift.io/acmeair-main-route created deployment.apps/acmeair-mainservice created service/acmeair-main-service created Acme Air Deployment complete. Access the application at acmeair.apps.kruize.cp.fyre.ibm.com/acmeair [root@kruize-inf acmeair]# oc get pods NAME READY STATUS RESTARTS AGE acmeair-authservice-55648d8445-4k4r4 1/1 Running 0 57m acmeair-booking-db-6ffc689d55-kxs9r 1/1 Running 0 57m acmeair-bookingservice-76fc8f7f84-5vsml 1/1 Running 0 57m acmeair-customer-db-ff6bcc8fc-qzxxp 1/1 Running 0 57m acmeair-customerservice-77b69b8f58-wvwrb 1/1 Running 0 57m acmeair-flight-db-6f9ff4dd6f-zczrl 1/1 Running 0 57m acmeair-flightservice-7d47988b8b-hwqjg 1/1 Running 0 57m acmeair-mainservice-7f7ff489d-qhss7 1/1 Running 0 57m In order to monitor your application, label the pod as mentioned in Step5: [root@kruize-inf acmeair]# oc label pod -n acme acmeair-customer-db-ff6bcc8fc-qzxxp app.kubernetes.io/name=acmeair-customer-db pod/acmeair-customer-db-ff6bcc8fc-qzxxp labeled List recommendations: [root@kruize-inf acmeair]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=acmeair-customer-db [ { \"application_name\": \"acmeair-customer-db\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } } ] NOTE: The above outputs are before adding any load on the application. After adding load: curl http://acmeair.apps.kruize.cp.fyre.ibm.com:6443/booking/loader/load curl http://acmeair.apps.kruize.cp.fyre.ibm.com:6443/flight/loader/load curl http://acmeair.apps.kruize.cp.fyre.ibm.com:6443/customer/loader/load?numCustomers=100000 To list recommendations for single application: [root@kruize-inf acmeair]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=acmeair-customer-db [ { \"application_name\": \"acmeair-customer-db\", \"resources\": { \"requests\": { \"memory\": \"194.6M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"233.5M\", \"cpu\": 1.0 } } } ] List of recommendations on all the applications: [root@kruize-inf acmeair]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?listApplications [ { \"application_name\": \"kube-state-metrics\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"node-exporter\", \"resources\": { \"requests\": { \"memory\": \"125.8M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"151.4M\", \"cpu\": 1.0 } } }, { \"application_name\": \"kruize\", \"resources\": { \"requests\": { \"memory\": \"175.9M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"278.1M\", \"cpu\": 1.0 } } }, { \"application_name\": \"acmeair-customer-db\", \"resources\": { \"requests\": { \"memory\": \"194.6M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"233.5M\", \"cpu\": 1.0 } } }, { \"application_name\": \"thanos-querier\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } }, { \"application_name\": \"prometheus-operator\", \"resources\": { \"requests\": { \"memory\": \"134.1M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"161.6M\", \"cpu\": 1.0 } } } ]","title":"1. To monitor deployment for an acmeair application:"},{"location":"kruize/#2-to-monitor-osd-pod-in-openshift-container-storage","text":"[root@kruize-inf ~]# oc get pods -n openshift-storage |grep osd NAME READY STATUS RESTARTS AGE rook-ceph-osd-0-6499496d58-pwml8 1/1 Running 0 12m rook-ceph-osd-1-5bd4d44b6f-dd6nq 1/1 Running 22 2d15h rook-ceph-osd-2-655f579dc7-65c7p 1/1 Running 0 141m Label the osd pod which you want to monitor: [root@kruize-inf ~]# oc label pod rook-ceph-osd-1-5bd4d44b6f-dd6nq -n openshift-storage app.kubernetes.io/name=rook-ceph-osd-1 pod/rook-ceph-osd-1-5bd4d44b6f-dd6nq labeled View Recommendations: Before adding load: [root@kruize-inf ~]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=rook-ceph-osd-1 [ { \"application_name\": \"rook-ceph-osd-1\", \"resources\": { \"requests\": { \"memory\": \"0.0M\", \"cpu\": 0.0 }, \"limits\": { \"memory\": \"0.0M\", \"cpu\": 0.0 } } } ] After running some tests/adding load: [root@kruize-inf ~]# curl http://kruize-openshift-monitoring.apps.kruize.cp.fyre.ibm.com/recommendations?application_name=rook-ceph-osd-1 [ { \"application_name\": \"rook-ceph-osd-1\", \"resources\": { \"requests\": { \"memory\": \"3427.2M\", \"cpu\": 0.5 }, \"limits\": { \"memory\": \"6415.4M\", \"cpu\": 1.0 } } } ]","title":"2. To monitor osd pod in Openshift Container Storage:"},{"location":"powervs-faq/","text":"This page has moved to https://ocp-power-automation.github.io/ocp-powervm-faq/docs/powervs-faq/ where all future updates will be made. Please update your bookmark. Thanks!!","title":"Powervs faq"},{"location":"powervs-faq/#this-page-has-moved-to-httpsocp-power-automationgithubioocp-powervm-faqdocspowervs-faq-where-all-future-updates-will-be-made-please-update-your-bookmark-thanks","text":"","title":"This page has moved to https://ocp-power-automation.github.io/ocp-powervm-faq/docs/powervs-faq/ where all future updates will be made. Please update your bookmark. Thanks!!"}]}